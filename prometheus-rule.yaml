Name:         prometheus-k8s-custom-alerting-rules
Namespace:    default
Labels:       prometheus=prometheus-service
              role=alert-rules
Annotations:  prometheus-operator-validated: true
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-26T19:45:36Z
  Generation:          2
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:kubectl.kubernetes.io/last-applied-configuration:
        f:labels:
          .:
          f:prometheus:
          f:role:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"custom statistic"}:
            .:
            f:name:
            f:rules:
    Manager:         kubectl-client-side-apply
    Operation:       Update
    Time:            2023-02-26T20:07:27Z
  Resource Version:  222313
  UID:               f40e231c-df8f-4799-abcb-56e8c892fecf
Spec:
  Groups:
    Name:  custom statistic
    Rules:
      Alert:  Memory percentage is >1%
      Annotations:
        Description:  Node memory utilization is > 95%
        Message:      Out of memory (instance {{ $labels.node }})
      Expr:           label_replace(sum(kube_metrics_server_nodes_mem) by (exported_instance), "node", "$1", "exported_instance", "(.*)")  / on(node) ((sum(kube_node_status_allocatable_memory_bytes) by (node)) / 1024) * 100 > 1
      For:            10m
      Labels:
        Resource:  {{ $labels.node }}
        Severity:  P3
Events:            <none>


Name:         prometheus-kube-prometheus-alertmanager.rules
Namespace:    default
Labels:       app=kube-prometheus-stack
              app.kubernetes.io/instance=prometheus
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/part-of=kube-prometheus-stack
              app.kubernetes.io/version=45.2.0
              chart=kube-prometheus-stack-45.2.0
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-22T21:15:22Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"alertmanager.rules"}:
            .:
            f:name:
            f:rules:
    Manager:         helm
    Operation:       Update
    Time:            2023-02-22T21:15:21Z
  Resource Version:  675
  UID:               0f234efe-025f-47fe-95e9-85516ab7edfc
Spec:
  Groups:
    Name:  alertmanager.rules
    Rules:
      Alert:  AlertmanagerFailedReload
      Annotations:
        Description:  Configuration has failed to load for {{ $labels.namespace }}/{{ $labels.pod}}.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerfailedreload
        Summary:      Reloading an Alertmanager configuration has failed.
      Expr:           # Without max_over_time, failed scrapes could create false negatives, see
# https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
max_over_time(alertmanager_config_last_reload_successful{job="prometheus-kube-prometheus-alertmanager",namespace="default"}[5m]) == 0
      For:  10m
      Labels:
        Severity:  critical
      Alert:       AlertmanagerMembersInconsistent
      Annotations:
        Description:  Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} has only found {{ $value }} members of the {{$labels.job}} cluster.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagermembersinconsistent
        Summary:      A member of an Alertmanager cluster has not found all other cluster members.
      Expr:           # Without max_over_time, failed scrapes could create false negatives, see
# https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
  max_over_time(alertmanager_cluster_members{job="prometheus-kube-prometheus-alertmanager",namespace="default"}[5m])
< on (namespace,service) group_left
  count by (namespace,service) (max_over_time(alertmanager_cluster_members{job="prometheus-kube-prometheus-alertmanager",namespace="default"}[5m]))
      For:  15m
      Labels:
        Severity:  critical
      Alert:       AlertmanagerFailedToSendAlerts
      Annotations:
        Description:  Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} failed to send {{ $value | humanizePercentage }} of notifications to {{ $labels.integration }}.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerfailedtosendalerts
        Summary:      An Alertmanager instance failed to send notifications.
      Expr:           (
  rate(alertmanager_notifications_failed_total{job="prometheus-kube-prometheus-alertmanager",namespace="default"}[5m])
/
  rate(alertmanager_notifications_total{job="prometheus-kube-prometheus-alertmanager",namespace="default"}[5m])
)
> 0.01
      For:  5m
      Labels:
        Severity:  warning
      Alert:       AlertmanagerClusterFailedToSendAlerts
      Annotations:
        Description:  The minimum notification failure rate to {{ $labels.integration }} sent from any instance in the {{$labels.job}} cluster is {{ $value | humanizePercentage }}.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclusterfailedtosendalerts
        Summary:      All Alertmanager instances in a cluster failed to send notifications to a critical integration.
      Expr:           min by (namespace,service, integration) (
  rate(alertmanager_notifications_failed_total{job="prometheus-kube-prometheus-alertmanager",namespace="default", integration=~`.*`}[5m])
/
  rate(alertmanager_notifications_total{job="prometheus-kube-prometheus-alertmanager",namespace="default", integration=~`.*`}[5m])
)
> 0.01
      For:  5m
      Labels:
        Severity:  critical
      Alert:       AlertmanagerClusterFailedToSendAlerts
      Annotations:
        Description:  The minimum notification failure rate to {{ $labels.integration }} sent from any instance in the {{$labels.job}} cluster is {{ $value | humanizePercentage }}.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclusterfailedtosendalerts
        Summary:      All Alertmanager instances in a cluster failed to send notifications to a non-critical integration.
      Expr:           min by (namespace,service, integration) (
  rate(alertmanager_notifications_failed_total{job="prometheus-kube-prometheus-alertmanager",namespace="default", integration!~`.*`}[5m])
/
  rate(alertmanager_notifications_total{job="prometheus-kube-prometheus-alertmanager",namespace="default", integration!~`.*`}[5m])
)
> 0.01
      For:  5m
      Labels:
        Severity:  warning
      Alert:       AlertmanagerConfigInconsistent
      Annotations:
        Description:  Alertmanager instances within the {{$labels.job}} cluster have different configurations.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerconfiginconsistent
        Summary:      Alertmanager instances within the same cluster have different configurations.
      Expr:           count by (namespace,service) (
  count_values by (namespace,service) ("config_hash", alertmanager_config_hash{job="prometheus-kube-prometheus-alertmanager",namespace="default"})
)
!= 1
      For:  20m
      Labels:
        Severity:  critical
      Alert:       AlertmanagerClusterDown
      Annotations:
        Description:  {{ $value | humanizePercentage }} of Alertmanager instances within the {{$labels.job}} cluster have been up for less than half of the last 5m.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclusterdown
        Summary:      Half or more of the Alertmanager instances within the same cluster are down.
      Expr:           (
  count by (namespace,service) (
    avg_over_time(up{job="prometheus-kube-prometheus-alertmanager",namespace="default"}[5m]) < 0.5
  )
/
  count by (namespace,service) (
    up{job="prometheus-kube-prometheus-alertmanager",namespace="default"}
  )
)
>= 0.5
      For:  5m
      Labels:
        Severity:  critical
      Alert:       AlertmanagerClusterCrashlooping
      Annotations:
        Description:  {{ $value | humanizePercentage }} of Alertmanager instances within the {{$labels.job}} cluster have restarted at least 5 times in the last 10m.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclustercrashlooping
        Summary:      Half or more of the Alertmanager instances within the same cluster are crashlooping.
      Expr:           (
  count by (namespace,service) (
    changes(process_start_time_seconds{job="prometheus-kube-prometheus-alertmanager",namespace="default"}[10m]) > 4
  )
/
  count by (namespace,service) (
    up{job="prometheus-kube-prometheus-alertmanager",namespace="default"}
  )
)
>= 0.5
      For:  5m
      Labels:
        Severity:  critical
Events:            <none>


Name:         prometheus-kube-prometheus-config-reloaders
Namespace:    default
Labels:       app=kube-prometheus-stack
              app.kubernetes.io/instance=prometheus
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/part-of=kube-prometheus-stack
              app.kubernetes.io/version=45.2.0
              chart=kube-prometheus-stack-45.2.0
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-22T21:15:22Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"config-reloaders"}:
            .:
            f:name:
            f:rules:
    Manager:         helm
    Operation:       Update
    Time:            2023-02-22T21:15:21Z
  Resource Version:  669
  UID:               db8d28db-1066-4ccf-9cfe-f348f5e5b38b
Spec:
  Groups:
    Name:  config-reloaders
    Rules:
      Alert:  ConfigReloaderSidecarErrors
      Annotations:
        Description:  Errors encountered while the {{$labels.pod}} config-reloader sidecar attempts to sync config in {{$labels.namespace}} namespace.
As a result, configuration for service running in {{$labels.pod}} may be stale and cannot be updated anymore.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/configreloadersidecarerrors
        Summary:      config-reloader sidecar has not had a successful reload for 10m
      Expr:           max_over_time(reloader_last_reload_successful{namespace=~".+"}[5m]) == 0
      For:            10m
      Labels:
        Severity:  warning
Events:            <none>


Name:         prometheus-kube-prometheus-etcd
Namespace:    default
Labels:       app=kube-prometheus-stack
              app.kubernetes.io/instance=prometheus
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/part-of=kube-prometheus-stack
              app.kubernetes.io/version=45.2.0
              chart=kube-prometheus-stack-45.2.0
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-22T21:15:22Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"etcd"}:
            .:
            f:name:
            f:rules:
    Manager:         helm
    Operation:       Update
    Time:            2023-02-22T21:15:21Z
  Resource Version:  687
  UID:               d701962c-ab2e-4edc-a454-9dcd004e17f7
Spec:
  Groups:
    Name:  etcd
    Rules:
      Alert:  etcdMembersDown
      Annotations:
        Description:  etcd cluster "{{ $labels.job }}": members are down ({{ $value }}).
        Summary:      etcd cluster members are down.
      Expr:           max without (endpoint) (
  sum without (instance) (up{job=~".*etcd.*"} == bool 0)
or
  count without (To) (
    sum without (instance) (rate(etcd_network_peer_sent_failures_total{job=~".*etcd.*"}[120s])) > 0.01
  )
)
> 0
      For:  10m
      Labels:
        Severity:  critical
      Alert:       etcdInsufficientMembers
      Annotations:
        Description:  etcd cluster "{{ $labels.job }}": insufficient members ({{ $value }}).
        Summary:      etcd cluster has insufficient number of members.
      Expr:           sum(up{job=~".*etcd.*"} == bool 1) without (instance) < ((count(up{job=~".*etcd.*"}) without (instance) + 1) / 2)
      For:            3m
      Labels:
        Severity:  critical
      Alert:       etcdNoLeader
      Annotations:
        Description:  etcd cluster "{{ $labels.job }}": member {{ $labels.instance }} has no leader.
        Summary:      etcd cluster has no leader.
      Expr:           etcd_server_has_leader{job=~".*etcd.*"} == 0
      For:            1m
      Labels:
        Severity:  critical
      Alert:       etcdHighNumberOfLeaderChanges
      Annotations:
        Description:  etcd cluster "{{ $labels.job }}": {{ $value }} leader changes within the last 15 minutes. Frequent elections may be a sign of insufficient resources, high network latency, or disruptions by other components and should be investigated.
        Summary:      etcd cluster has high number of leader changes.
      Expr:           increase((max without (instance) (etcd_server_leader_changes_seen_total{job=~".*etcd.*"}) or 0*absent(etcd_server_leader_changes_seen_total{job=~".*etcd.*"}))[15m:1m]) >= 4
      For:            5m
      Labels:
        Severity:  warning
      Alert:       etcdHighNumberOfFailedGRPCRequests
      Annotations:
        Description:  etcd cluster "{{ $labels.job }}": {{ $value }}% of requests for {{ $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}.
        Summary:      etcd cluster has high number of failed grpc requests.
      Expr:           100 * sum(rate(grpc_server_handled_total{job=~".*etcd.*", grpc_code=~"Unknown|FailedPrecondition|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded"}[5m])) without (grpc_type, grpc_code)
  /
sum(rate(grpc_server_handled_total{job=~".*etcd.*"}[5m])) without (grpc_type, grpc_code)
  > 1
      For:  10m
      Labels:
        Severity:  warning
      Alert:       etcdHighNumberOfFailedGRPCRequests
      Annotations:
        Description:  etcd cluster "{{ $labels.job }}": {{ $value }}% of requests for {{ $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}.
        Summary:      etcd cluster has high number of failed grpc requests.
      Expr:           100 * sum(rate(grpc_server_handled_total{job=~".*etcd.*", grpc_code=~"Unknown|FailedPrecondition|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded"}[5m])) without (grpc_type, grpc_code)
  /
sum(rate(grpc_server_handled_total{job=~".*etcd.*"}[5m])) without (grpc_type, grpc_code)
  > 5
      For:  5m
      Labels:
        Severity:  critical
      Alert:       etcdGRPCRequestsSlow
      Annotations:
        Description:  etcd cluster "{{ $labels.job }}": 99th percentile of gRPC requests is {{ $value }}s on etcd instance {{ $labels.instance }} for {{ $labels.grpc_method }} method.
        Summary:      etcd grpc requests are slow
      Expr:           histogram_quantile(0.99, sum(rate(grpc_server_handling_seconds_bucket{job=~".*etcd.*", grpc_method!="Defragment", grpc_type="unary"}[5m])) without(grpc_type))
> 0.15
      For:  10m
      Labels:
        Severity:  critical
      Alert:       etcdMemberCommunicationSlow
      Annotations:
        Description:  etcd cluster "{{ $labels.job }}": member communication with {{ $labels.To }} is taking {{ $value }}s on etcd instance {{ $labels.instance }}.
        Summary:      etcd cluster member communication is slow.
      Expr:           histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket{job=~".*etcd.*"}[5m]))
> 0.15
      For:  10m
      Labels:
        Severity:  warning
      Alert:       etcdHighNumberOfFailedProposals
      Annotations:
        Description:  etcd cluster "{{ $labels.job }}": {{ $value }} proposal failures within the last 30 minutes on etcd instance {{ $labels.instance }}.
        Summary:      etcd cluster has high number of proposal failures.
      Expr:           rate(etcd_server_proposals_failed_total{job=~".*etcd.*"}[15m]) > 5
      For:            15m
      Labels:
        Severity:  warning
      Alert:       etcdHighFsyncDurations
      Annotations:
        Description:  etcd cluster "{{ $labels.job }}": 99th percentile fsync durations are {{ $value }}s on etcd instance {{ $labels.instance }}.
        Summary:      etcd cluster 99th percentile fsync durations are too high.
      Expr:           histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~".*etcd.*"}[5m]))
> 0.5
      For:  10m
      Labels:
        Severity:  warning
      Alert:       etcdHighFsyncDurations
      Annotations:
        Description:  etcd cluster "{{ $labels.job }}": 99th percentile fsync durations are {{ $value }}s on etcd instance {{ $labels.instance }}.
        Summary:      etcd cluster 99th percentile fsync durations are too high.
      Expr:           histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~".*etcd.*"}[5m]))
> 1
      For:  10m
      Labels:
        Severity:  critical
      Alert:       etcdHighCommitDurations
      Annotations:
        Description:  etcd cluster "{{ $labels.job }}": 99th percentile commit durations {{ $value }}s on etcd instance {{ $labels.instance }}.
        Summary:      etcd cluster 99th percentile commit durations are too high.
      Expr:           histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket{job=~".*etcd.*"}[5m]))
> 0.25
      For:  10m
      Labels:
        Severity:  warning
      Alert:       etcdDatabaseQuotaLowSpace
      Annotations:
        Description:  etcd cluster "{{ $labels.job }}": database size exceeds the defined quota on etcd instance {{ $labels.instance }}, please defrag or increase the quota as the writes to etcd will be disabled when it is full.
        Summary:      etcd cluster database is running full.
      Expr:           (last_over_time(etcd_mvcc_db_total_size_in_bytes[5m]) / last_over_time(etcd_server_quota_backend_bytes[5m]))*100 > 95
      For:            10m
      Labels:
        Severity:  critical
      Alert:       etcdExcessiveDatabaseGrowth
      Annotations:
        Description:  etcd cluster "{{ $labels.job }}": Predicting running out of disk space in the next four hours, based on write observations within the past four hours on etcd instance {{ $labels.instance }}, please check as it might be disruptive.
        Summary:      etcd cluster database growing very fast.
      Expr:           predict_linear(etcd_mvcc_db_total_size_in_bytes[4h], 4*60*60) > etcd_server_quota_backend_bytes
      For:            10m
      Labels:
        Severity:  warning
      Alert:       etcdDatabaseHighFragmentationRatio
      Annotations:
        Description:  etcd cluster "{{ $labels.job }}": database size in use on instance {{ $labels.instance }} is {{ $value | humanizePercentage }} of the actual allocated disk space, please run defragmentation (e.g. etcdctl defrag) to retrieve the unused fragmented disk space.
        runbook_url:  https://etcd.io/docs/v3.5/op-guide/maintenance/#defragmentation
        Summary:      etcd database size in use is less than 50% of the actual allocated storage.
      Expr:           (last_over_time(etcd_mvcc_db_total_size_in_use_in_bytes[5m]) / last_over_time(etcd_mvcc_db_total_size_in_bytes[5m])) < 0.5
      For:            10m
      Labels:
        Severity:  warning
Events:            <none>


Name:         prometheus-kube-prometheus-general.rules
Namespace:    default
Labels:       app=kube-prometheus-stack
              app.kubernetes.io/instance=prometheus
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/part-of=kube-prometheus-stack
              app.kubernetes.io/version=45.2.0
              chart=kube-prometheus-stack-45.2.0
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-22T21:15:22Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"general.rules"}:
            .:
            f:name:
            f:rules:
    Manager:         helm
    Operation:       Update
    Time:            2023-02-22T21:15:21Z
  Resource Version:  679
  UID:               55bdde78-519e-4544-b83e-d5e442bc811c
Spec:
  Groups:
    Name:  general.rules
    Rules:
      Alert:  TargetDown
      Annotations:
        Description:  {{ printf "%.4g" $value }}% of the {{ $labels.job }}/{{ $labels.service }} targets in {{ $labels.namespace }} namespace are down.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/general/targetdown
        Summary:      One or more targets are unreachable.
      Expr:           100 * (count(up == 0) BY (job, namespace, service) / count(up) BY (job, namespace, service)) > 10
      For:            10m
      Labels:
        Severity:  warning
      Alert:       Watchdog
      Annotations:
        Description:  This is an alert meant to ensure that the entire alerting pipeline is functional.
This alert is always firing, therefore it should always be firing in Alertmanager
and always fire against a receiver. There are integrations with various notification
mechanisms that send a notification when this alert is not firing. For example the
"DeadMansSnitch" integration in PagerDuty.

        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/general/watchdog
        Summary:      An alert that should always be firing to certify that Alertmanager is working properly.
      Expr:           vector(1)
      Labels:
        Severity:  none
      Alert:       InfoInhibitor
      Annotations:
        Description:  This is an alert that is used to inhibit info alerts.
By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with
other alerts.
This alert fires whenever there's a severity="info" alert, and stops firing when another alert with a
severity of 'warning' or 'critical' starts firing on the same namespace.
This alert should be routed to a null receiver and configured to inhibit alerts with severity="info".

        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/general/infoinhibitor
        Summary:      Info-level alert inhibition.
      Expr:           ALERTS{severity = "info"} == 1 unless on(namespace) ALERTS{alertname != "InfoInhibitor", severity =~ "warning|critical", alertstate="firing"} == 1
      Labels:
        Severity:  none
Events:            <none>


Name:         prometheus-kube-prometheus-k8s.rules
Namespace:    default
Labels:       app=kube-prometheus-stack
              app.kubernetes.io/instance=prometheus
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/part-of=kube-prometheus-stack
              app.kubernetes.io/version=45.2.0
              chart=kube-prometheus-stack-45.2.0
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-22T21:15:22Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"k8s.rules"}:
            .:
            f:name:
            f:rules:
    Manager:         helm
    Operation:       Update
    Time:            2023-02-22T21:15:21Z
  Resource Version:  695
  UID:               c85fa6be-e674-4b74-be3b-aa54906310b9
Spec:
  Groups:
    Name:  k8s.rules
    Rules:
      Expr:  sum by (cluster, namespace, pod, container) (
  irate(container_cpu_usage_seconds_total{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}[5m])
) * on (cluster, namespace, pod) group_left(node) topk by (cluster, namespace, pod) (
  1, max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
)
      Record:  node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate
      Expr:    container_memory_working_set_bytes{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
* on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
  max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
)
      Record:  node_namespace_pod_container:container_memory_working_set_bytes
      Expr:    container_memory_rss{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
* on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
  max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
)
      Record:  node_namespace_pod_container:container_memory_rss
      Expr:    container_memory_cache{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
* on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
  max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
)
      Record:  node_namespace_pod_container:container_memory_cache
      Expr:    container_memory_swap{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
* on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
  max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
)
      Record:  node_namespace_pod_container:container_memory_swap
      Expr:    kube_pod_container_resource_requests{resource="memory",job="kube-state-metrics"}  * on (namespace, pod, cluster)
group_left() max by (namespace, pod, cluster) (
  (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
)
      Record:  cluster:namespace:pod_memory:active:kube_pod_container_resource_requests
      Expr:    sum by (namespace, cluster) (
    sum by (namespace, pod, cluster) (
        max by (namespace, pod, container, cluster) (
          kube_pod_container_resource_requests{resource="memory",job="kube-state-metrics"}
        ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
          kube_pod_status_phase{phase=~"Pending|Running"} == 1
        )
    )
)
      Record:  namespace_memory:kube_pod_container_resource_requests:sum
      Expr:    kube_pod_container_resource_requests{resource="cpu",job="kube-state-metrics"}  * on (namespace, pod, cluster)
group_left() max by (namespace, pod, cluster) (
  (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
)
      Record:  cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests
      Expr:    sum by (namespace, cluster) (
    sum by (namespace, pod, cluster) (
        max by (namespace, pod, container, cluster) (
          kube_pod_container_resource_requests{resource="cpu",job="kube-state-metrics"}
        ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
          kube_pod_status_phase{phase=~"Pending|Running"} == 1
        )
    )
)
      Record:  namespace_cpu:kube_pod_container_resource_requests:sum
      Expr:    kube_pod_container_resource_limits{resource="memory",job="kube-state-metrics"}  * on (namespace, pod, cluster)
group_left() max by (namespace, pod, cluster) (
  (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
)
      Record:  cluster:namespace:pod_memory:active:kube_pod_container_resource_limits
      Expr:    sum by (namespace, cluster) (
    sum by (namespace, pod, cluster) (
        max by (namespace, pod, container, cluster) (
          kube_pod_container_resource_limits{resource="memory",job="kube-state-metrics"}
        ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
          kube_pod_status_phase{phase=~"Pending|Running"} == 1
        )
    )
)
      Record:  namespace_memory:kube_pod_container_resource_limits:sum
      Expr:    kube_pod_container_resource_limits{resource="cpu",job="kube-state-metrics"}  * on (namespace, pod, cluster)
group_left() max by (namespace, pod, cluster) (
 (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
 )
      Record:  cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits
      Expr:    sum by (namespace, cluster) (
    sum by (namespace, pod, cluster) (
        max by (namespace, pod, container, cluster) (
          kube_pod_container_resource_limits{resource="cpu",job="kube-state-metrics"}
        ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
          kube_pod_status_phase{phase=~"Pending|Running"} == 1
        )
    )
)
      Record:  namespace_cpu:kube_pod_container_resource_limits:sum
      Expr:    max by (cluster, namespace, workload, pod) (
  label_replace(
    label_replace(
      kube_pod_owner{job="kube-state-metrics", owner_kind="ReplicaSet"},
      "replicaset", "$1", "owner_name", "(.*)"
    ) * on(replicaset, namespace) group_left(owner_name) topk by(replicaset, namespace) (
      1, max by (replicaset, namespace, owner_name) (
        kube_replicaset_owner{job="kube-state-metrics"}
      )
    ),
    "workload", "$1", "owner_name", "(.*)"
  )
)
      Labels:
        workload_type:  deployment
      Record:           namespace_workload_pod:kube_pod_owner:relabel
      Expr:             max by (cluster, namespace, workload, pod) (
  label_replace(
    kube_pod_owner{job="kube-state-metrics", owner_kind="DaemonSet"},
    "workload", "$1", "owner_name", "(.*)"
  )
)
      Labels:
        workload_type:  daemonset
      Record:           namespace_workload_pod:kube_pod_owner:relabel
      Expr:             max by (cluster, namespace, workload, pod) (
  label_replace(
    kube_pod_owner{job="kube-state-metrics", owner_kind="StatefulSet"},
    "workload", "$1", "owner_name", "(.*)"
  )
)
      Labels:
        workload_type:  statefulset
      Record:           namespace_workload_pod:kube_pod_owner:relabel
      Expr:             max by (cluster, namespace, workload, pod) (
  label_replace(
    kube_pod_owner{job="kube-state-metrics", owner_kind="Job"},
    "workload", "$1", "owner_name", "(.*)"
  )
)
      Labels:
        workload_type:  job
      Record:           namespace_workload_pod:kube_pod_owner:relabel
Events:                 <none>


Name:         prometheus-kube-prometheus-kube-apiserver-availability.rules
Namespace:    default
Labels:       app=kube-prometheus-stack
              app.kubernetes.io/instance=prometheus
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/part-of=kube-prometheus-stack
              app.kubernetes.io/version=45.2.0
              chart=kube-prometheus-stack-45.2.0
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-22T21:15:22Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"kube-apiserver-availability.rules"}:
            .:
            f:interval:
            f:name:
            f:rules:
    Manager:         helm
    Operation:       Update
    Time:            2023-02-22T21:15:21Z
  Resource Version:  690
  UID:               f19b5747-a41b-48b9-a950-313b6b59e069
Spec:
  Groups:
    Interval:  3m
    Name:      kube-apiserver-availability.rules
    Rules:
      Expr:    avg_over_time(code_verb:apiserver_request_total:increase1h[30d]) * 24 * 30
      Record:  code_verb:apiserver_request_total:increase30d
      Expr:    sum by (cluster, code) (code_verb:apiserver_request_total:increase30d{verb=~"LIST|GET"})
      Labels:
        Verb:  read
      Record:  code:apiserver_request_total:increase30d
      Expr:    sum by (cluster, code) (code_verb:apiserver_request_total:increase30d{verb=~"POST|PUT|PATCH|DELETE"})
      Labels:
        Verb:  write
      Record:  code:apiserver_request_total:increase30d
      Expr:    sum by (cluster, verb, scope) (increase(apiserver_request_slo_duration_seconds_count[1h]))
      Record:  cluster_verb_scope:apiserver_request_slo_duration_seconds_count:increase1h
      Expr:    sum by (cluster, verb, scope) (avg_over_time(cluster_verb_scope:apiserver_request_slo_duration_seconds_count:increase1h[30d]) * 24 * 30)
      Record:  cluster_verb_scope:apiserver_request_slo_duration_seconds_count:increase30d
      Expr:    sum by (cluster, verb, scope, le) (increase(apiserver_request_slo_duration_seconds_bucket[1h]))
      Record:  cluster_verb_scope_le:apiserver_request_slo_duration_seconds_bucket:increase1h
      Expr:    sum by (cluster, verb, scope, le) (avg_over_time(cluster_verb_scope_le:apiserver_request_slo_duration_seconds_bucket:increase1h[30d]) * 24 * 30)
      Record:  cluster_verb_scope_le:apiserver_request_slo_duration_seconds_bucket:increase30d
      Expr:    1 - (
  (
    # write too slow
    sum by (cluster) (cluster_verb_scope:apiserver_request_slo_duration_seconds_count:increase30d{verb=~"POST|PUT|PATCH|DELETE"})
    -
    sum by (cluster) (cluster_verb_scope_le:apiserver_request_slo_duration_seconds_bucket:increase30d{verb=~"POST|PUT|PATCH|DELETE",le="1"})
  ) +
  (
    # read too slow
    sum by (cluster) (cluster_verb_scope:apiserver_request_slo_duration_seconds_count:increase30d{verb=~"LIST|GET"})
    -
    (
      (
        sum by (cluster) (cluster_verb_scope_le:apiserver_request_slo_duration_seconds_bucket:increase30d{verb=~"LIST|GET",scope=~"resource|",le="1"})
        or
        vector(0)
      )
      +
      sum by (cluster) (cluster_verb_scope_le:apiserver_request_slo_duration_seconds_bucket:increase30d{verb=~"LIST|GET",scope="namespace",le="5"})
      +
      sum by (cluster) (cluster_verb_scope_le:apiserver_request_slo_duration_seconds_bucket:increase30d{verb=~"LIST|GET",scope="cluster",le="30"})
    )
  ) +
  # errors
  sum by (cluster) (code:apiserver_request_total:increase30d{code=~"5.."} or vector(0))
)
/
sum by (cluster) (code:apiserver_request_total:increase30d)
      Labels:
        Verb:  all
      Record:  apiserver_request:availability30d
      Expr:    1 - (
  sum by (cluster) (cluster_verb_scope:apiserver_request_slo_duration_seconds_count:increase30d{verb=~"LIST|GET"})
  -
  (
    # too slow
    (
      sum by (cluster) (cluster_verb_scope_le:apiserver_request_slo_duration_seconds_bucket:increase30d{verb=~"LIST|GET",scope=~"resource|",le="1"})
      or
      vector(0)
    )
    +
    sum by (cluster) (cluster_verb_scope_le:apiserver_request_slo_duration_seconds_bucket:increase30d{verb=~"LIST|GET",scope="namespace",le="5"})
    +
    sum by (cluster) (cluster_verb_scope_le:apiserver_request_slo_duration_seconds_bucket:increase30d{verb=~"LIST|GET",scope="cluster",le="30"})
  )
  +
  # errors
  sum by (cluster) (code:apiserver_request_total:increase30d{verb="read",code=~"5.."} or vector(0))
)
/
sum by (cluster) (code:apiserver_request_total:increase30d{verb="read"})
      Labels:
        Verb:  read
      Record:  apiserver_request:availability30d
      Expr:    1 - (
  (
    # too slow
    sum by (cluster) (cluster_verb_scope:apiserver_request_slo_duration_seconds_count:increase30d{verb=~"POST|PUT|PATCH|DELETE"})
    -
    sum by (cluster) (cluster_verb_scope_le:apiserver_request_slo_duration_seconds_bucket:increase30d{verb=~"POST|PUT|PATCH|DELETE",le="1"})
  )
  +
  # errors
  sum by (cluster) (code:apiserver_request_total:increase30d{verb="write",code=~"5.."} or vector(0))
)
/
sum by (cluster) (code:apiserver_request_total:increase30d{verb="write"})
      Labels:
        Verb:  write
      Record:  apiserver_request:availability30d
      Expr:    sum by (cluster,code,resource) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[5m]))
      Labels:
        Verb:  read
      Record:  code_resource:apiserver_request_total:rate5m
      Expr:    sum by (cluster,code,resource) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m]))
      Labels:
        Verb:  write
      Record:  code_resource:apiserver_request_total:rate5m
      Expr:    sum by (cluster, code, verb) (increase(apiserver_request_total{job="apiserver",verb=~"LIST|GET|POST|PUT|PATCH|DELETE",code=~"2.."}[1h]))
      Record:  code_verb:apiserver_request_total:increase1h
      Expr:    sum by (cluster, code, verb) (increase(apiserver_request_total{job="apiserver",verb=~"LIST|GET|POST|PUT|PATCH|DELETE",code=~"3.."}[1h]))
      Record:  code_verb:apiserver_request_total:increase1h
      Expr:    sum by (cluster, code, verb) (increase(apiserver_request_total{job="apiserver",verb=~"LIST|GET|POST|PUT|PATCH|DELETE",code=~"4.."}[1h]))
      Record:  code_verb:apiserver_request_total:increase1h
      Expr:    sum by (cluster, code, verb) (increase(apiserver_request_total{job="apiserver",verb=~"LIST|GET|POST|PUT|PATCH|DELETE",code=~"5.."}[1h]))
      Record:  code_verb:apiserver_request_total:increase1h
Events:        <none>


Name:         prometheus-kube-prometheus-kube-apiserver-burnrate.rules
Namespace:    default
Labels:       app=kube-prometheus-stack
              app.kubernetes.io/instance=prometheus
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/part-of=kube-prometheus-stack
              app.kubernetes.io/version=45.2.0
              chart=kube-prometheus-stack-45.2.0
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-22T21:15:22Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"kube-apiserver-burnrate.rules"}:
            .:
            f:name:
            f:rules:
    Manager:         helm
    Operation:       Update
    Time:            2023-02-22T21:15:21Z
  Resource Version:  689
  UID:               e4dd072b-3f74-4bb6-a7fd-b7c8d40b2aa8
Spec:
  Groups:
    Name:  kube-apiserver-burnrate.rules
    Rules:
      Expr:  (
  (
    # too slow
    sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward"}[1d]))
    -
    (
      (
        sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope=~"resource|",le="1"}[1d]))
        or
        vector(0)
      )
      +
      sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="namespace",le="5"}[1d]))
      +
      sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="cluster",le="30"}[1d]))
    )
  )
  +
  # errors
  sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[1d]))
)
/
sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[1d]))
      Labels:
        Verb:  read
      Record:  apiserver_request:burnrate1d
      Expr:    (
  (
    # too slow
    sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward"}[1h]))
    -
    (
      (
        sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope=~"resource|",le="1"}[1h]))
        or
        vector(0)
      )
      +
      sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="namespace",le="5"}[1h]))
      +
      sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="cluster",le="30"}[1h]))
    )
  )
  +
  # errors
  sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[1h]))
)
/
sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[1h]))
      Labels:
        Verb:  read
      Record:  apiserver_request:burnrate1h
      Expr:    (
  (
    # too slow
    sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward"}[2h]))
    -
    (
      (
        sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope=~"resource|",le="1"}[2h]))
        or
        vector(0)
      )
      +
      sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="namespace",le="5"}[2h]))
      +
      sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="cluster",le="30"}[2h]))
    )
  )
  +
  # errors
  sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[2h]))
)
/
sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[2h]))
      Labels:
        Verb:  read
      Record:  apiserver_request:burnrate2h
      Expr:    (
  (
    # too slow
    sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward"}[30m]))
    -
    (
      (
        sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope=~"resource|",le="1"}[30m]))
        or
        vector(0)
      )
      +
      sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="namespace",le="5"}[30m]))
      +
      sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="cluster",le="30"}[30m]))
    )
  )
  +
  # errors
  sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[30m]))
)
/
sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[30m]))
      Labels:
        Verb:  read
      Record:  apiserver_request:burnrate30m
      Expr:    (
  (
    # too slow
    sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward"}[3d]))
    -
    (
      (
        sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope=~"resource|",le="1"}[3d]))
        or
        vector(0)
      )
      +
      sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="namespace",le="5"}[3d]))
      +
      sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="cluster",le="30"}[3d]))
    )
  )
  +
  # errors
  sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[3d]))
)
/
sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[3d]))
      Labels:
        Verb:  read
      Record:  apiserver_request:burnrate3d
      Expr:    (
  (
    # too slow
    sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward"}[5m]))
    -
    (
      (
        sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope=~"resource|",le="1"}[5m]))
        or
        vector(0)
      )
      +
      sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="namespace",le="5"}[5m]))
      +
      sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="cluster",le="30"}[5m]))
    )
  )
  +
  # errors
  sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[5m]))
)
/
sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[5m]))
      Labels:
        Verb:  read
      Record:  apiserver_request:burnrate5m
      Expr:    (
  (
    # too slow
    sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward"}[6h]))
    -
    (
      (
        sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope=~"resource|",le="1"}[6h]))
        or
        vector(0)
      )
      +
      sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="namespace",le="5"}[6h]))
      +
      sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="cluster",le="30"}[6h]))
    )
  )
  +
  # errors
  sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[6h]))
)
/
sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[6h]))
      Labels:
        Verb:  read
      Record:  apiserver_request:burnrate6h
      Expr:    (
  (
    # too slow
    sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward"}[1d]))
    -
    sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward",le="1"}[1d]))
  )
  +
  sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[1d]))
)
/
sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[1d]))
      Labels:
        Verb:  write
      Record:  apiserver_request:burnrate1d
      Expr:    (
  (
    # too slow
    sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward"}[1h]))
    -
    sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward",le="1"}[1h]))
  )
  +
  sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[1h]))
)
/
sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[1h]))
      Labels:
        Verb:  write
      Record:  apiserver_request:burnrate1h
      Expr:    (
  (
    # too slow
    sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward"}[2h]))
    -
    sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward",le="1"}[2h]))
  )
  +
  sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[2h]))
)
/
sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[2h]))
      Labels:
        Verb:  write
      Record:  apiserver_request:burnrate2h
      Expr:    (
  (
    # too slow
    sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward"}[30m]))
    -
    sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward",le="1"}[30m]))
  )
  +
  sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[30m]))
)
/
sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[30m]))
      Labels:
        Verb:  write
      Record:  apiserver_request:burnrate30m
      Expr:    (
  (
    # too slow
    sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward"}[3d]))
    -
    sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward",le="1"}[3d]))
  )
  +
  sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[3d]))
)
/
sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[3d]))
      Labels:
        Verb:  write
      Record:  apiserver_request:burnrate3d
      Expr:    (
  (
    # too slow
    sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward"}[5m]))
    -
    sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward",le="1"}[5m]))
  )
  +
  sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[5m]))
)
/
sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m]))
      Labels:
        Verb:  write
      Record:  apiserver_request:burnrate5m
      Expr:    (
  (
    # too slow
    sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward"}[6h]))
    -
    sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward",le="1"}[6h]))
  )
  +
  sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[6h]))
)
/
sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[6h]))
      Labels:
        Verb:  write
      Record:  apiserver_request:burnrate6h
Events:        <none>


Name:         prometheus-kube-prometheus-kube-apiserver-histogram.rules
Namespace:    default
Labels:       app=kube-prometheus-stack
              app.kubernetes.io/instance=prometheus
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/part-of=kube-prometheus-stack
              app.kubernetes.io/version=45.2.0
              chart=kube-prometheus-stack-45.2.0
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-22T21:15:22Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"kube-apiserver-histogram.rules"}:
            .:
            f:name:
            f:rules:
    Manager:         helm
    Operation:       Update
    Time:            2023-02-22T21:15:21Z
  Resource Version:  688
  UID:               77938f7c-bfb4-464f-879c-7e1f65a91767
Spec:
  Groups:
    Name:  kube-apiserver-histogram.rules
    Rules:
      Expr:  histogram_quantile(0.99, sum by (cluster, le, resource) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward"}[5m]))) > 0
      Labels:
        Quantile:  0.99
        Verb:      read
      Record:      cluster_quantile:apiserver_request_slo_duration_seconds:histogram_quantile
      Expr:        histogram_quantile(0.99, sum by (cluster, le, resource) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward"}[5m]))) > 0
      Labels:
        Quantile:  0.99
        Verb:      write
      Record:      cluster_quantile:apiserver_request_slo_duration_seconds:histogram_quantile
Events:            <none>


Name:         prometheus-kube-prometheus-kube-apiserver-slos
Namespace:    default
Labels:       app=kube-prometheus-stack
              app.kubernetes.io/instance=prometheus
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/part-of=kube-prometheus-stack
              app.kubernetes.io/version=45.2.0
              chart=kube-prometheus-stack-45.2.0
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-22T21:15:22Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"kube-apiserver-slos"}:
            .:
            f:name:
            f:rules:
    Manager:         helm
    Operation:       Update
    Time:            2023-02-22T21:15:21Z
  Resource Version:  680
  UID:               85b23f82-945b-433c-9e69-fa9cd6d4a233
Spec:
  Groups:
    Name:  kube-apiserver-slos
    Rules:
      Alert:  KubeAPIErrorBudgetBurn
      Annotations:
        Description:  The API server is burning too much error budget.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn
        Summary:      The API server is burning too much error budget.
      Expr:           sum(apiserver_request:burnrate1h) > (14.40 * 0.01000)
and
sum(apiserver_request:burnrate5m) > (14.40 * 0.01000)
      For:  2m
      Labels:
        Long:      1h
        Severity:  critical
        Short:     5m
      Alert:       KubeAPIErrorBudgetBurn
      Annotations:
        Description:  The API server is burning too much error budget.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn
        Summary:      The API server is burning too much error budget.
      Expr:           sum(apiserver_request:burnrate6h) > (6.00 * 0.01000)
and
sum(apiserver_request:burnrate30m) > (6.00 * 0.01000)
      For:  15m
      Labels:
        Long:      6h
        Severity:  critical
        Short:     30m
      Alert:       KubeAPIErrorBudgetBurn
      Annotations:
        Description:  The API server is burning too much error budget.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn
        Summary:      The API server is burning too much error budget.
      Expr:           sum(apiserver_request:burnrate1d) > (3.00 * 0.01000)
and
sum(apiserver_request:burnrate2h) > (3.00 * 0.01000)
      For:  1h
      Labels:
        Long:      1d
        Severity:  warning
        Short:     2h
      Alert:       KubeAPIErrorBudgetBurn
      Annotations:
        Description:  The API server is burning too much error budget.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn
        Summary:      The API server is burning too much error budget.
      Expr:           sum(apiserver_request:burnrate3d) > (1.00 * 0.01000)
and
sum(apiserver_request:burnrate6h) > (1.00 * 0.01000)
      For:  3h
      Labels:
        Long:      3d
        Severity:  warning
        Short:     6h
Events:            <none>


Name:         prometheus-kube-prometheus-kube-prometheus-general.rules
Namespace:    default
Labels:       app=kube-prometheus-stack
              app.kubernetes.io/instance=prometheus
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/part-of=kube-prometheus-stack
              app.kubernetes.io/version=45.2.0
              chart=kube-prometheus-stack-45.2.0
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-22T21:15:22Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"kube-prometheus-general.rules"}:
            .:
            f:name:
            f:rules:
    Manager:         helm
    Operation:       Update
    Time:            2023-02-22T21:15:21Z
  Resource Version:  672
  UID:               fa68ec7d-2911-46d7-bcdd-1d1a0fe523f7
Spec:
  Groups:
    Name:  kube-prometheus-general.rules
    Rules:
      Expr:    count without(instance, pod, node) (up == 1)
      Record:  count:up1
      Expr:    count without(instance, pod, node) (up == 0)
      Record:  count:up0
Events:        <none>


Name:         prometheus-kube-prometheus-kube-prometheus-node-recording.rules
Namespace:    default
Labels:       app=kube-prometheus-stack
              app.kubernetes.io/instance=prometheus
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/part-of=kube-prometheus-stack
              app.kubernetes.io/version=45.2.0
              chart=kube-prometheus-stack-45.2.0
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-22T21:15:22Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"kube-prometheus-node-recording.rules"}:
            .:
            f:name:
            f:rules:
    Manager:         helm
    Operation:       Update
    Time:            2023-02-22T21:15:21Z
  Resource Version:  671
  UID:               d744bafc-ee75-4e6a-a2a9-b102cf178ffc
Spec:
  Groups:
    Name:  kube-prometheus-node-recording.rules
    Rules:
      Expr:    sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal"}[3m])) BY (instance)
      Record:  instance:node_cpu:rate:sum
      Expr:    sum(rate(node_network_receive_bytes_total[3m])) BY (instance)
      Record:  instance:node_network_receive_bytes:rate:sum
      Expr:    sum(rate(node_network_transmit_bytes_total[3m])) BY (instance)
      Record:  instance:node_network_transmit_bytes:rate:sum
      Expr:    sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal"}[5m])) WITHOUT (cpu, mode) / ON(instance) GROUP_LEFT() count(sum(node_cpu_seconds_total) BY (instance, cpu)) BY (instance)
      Record:  instance:node_cpu:ratio
      Expr:    sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal"}[5m]))
      Record:  cluster:node_cpu:sum_rate5m
      Expr:    cluster:node_cpu:sum_rate5m / count(sum(node_cpu_seconds_total) BY (instance, cpu))
      Record:  cluster:node_cpu:ratio
Events:        <none>


Name:         prometheus-kube-prometheus-kube-scheduler.rules
Namespace:    default
Labels:       app=kube-prometheus-stack
              app.kubernetes.io/instance=prometheus
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/part-of=kube-prometheus-stack
              app.kubernetes.io/version=45.2.0
              chart=kube-prometheus-stack-45.2.0
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-22T21:15:22Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"kube-scheduler.rules"}:
            .:
            f:name:
            f:rules:
    Manager:         helm
    Operation:       Update
    Time:            2023-02-22T21:15:21Z
  Resource Version:  696
  UID:               cf820429-52c1-431c-acef-988122452b2a
Spec:
  Groups:
    Name:  kube-scheduler.rules
    Rules:
      Expr:  histogram_quantile(0.99, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
      Labels:
        Quantile:  0.99
      Record:      cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
      Expr:        histogram_quantile(0.99, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
      Labels:
        Quantile:  0.99
      Record:      cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
      Expr:        histogram_quantile(0.99, sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
      Labels:
        Quantile:  0.99
      Record:      cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
      Expr:        histogram_quantile(0.9, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
      Labels:
        Quantile:  0.9
      Record:      cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
      Expr:        histogram_quantile(0.9, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
      Labels:
        Quantile:  0.9
      Record:      cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
      Expr:        histogram_quantile(0.9, sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
      Labels:
        Quantile:  0.9
      Record:      cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
      Expr:        histogram_quantile(0.5, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
      Labels:
        Quantile:  0.5
      Record:      cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
      Expr:        histogram_quantile(0.5, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
      Labels:
        Quantile:  0.5
      Record:      cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
      Expr:        histogram_quantile(0.5, sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
      Labels:
        Quantile:  0.5
      Record:      cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
Events:            <none>


Name:         prometheus-kube-prometheus-kube-state-metrics
Namespace:    default
Labels:       app=kube-prometheus-stack
              app.kubernetes.io/instance=prometheus
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/part-of=kube-prometheus-stack
              app.kubernetes.io/version=45.2.0
              chart=kube-prometheus-stack-45.2.0
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-22T21:15:22Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"kube-state-metrics"}:
            .:
            f:name:
            f:rules:
    Manager:         helm
    Operation:       Update
    Time:            2023-02-22T21:15:21Z
  Resource Version:  684
  UID:               6b1ef893-b2fa-4190-b367-7f22315871b7
Spec:
  Groups:
    Name:  kube-state-metrics
    Rules:
      Alert:  KubeStateMetricsListErrors
      Annotations:
        Description:  kube-state-metrics is experiencing errors at an elevated rate in list operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricslisterrors
        Summary:      kube-state-metrics is experiencing errors in list operations.
      Expr:           (sum(rate(kube_state_metrics_list_total{job="kube-state-metrics",result="error"}[5m]))
  /
sum(rate(kube_state_metrics_list_total{job="kube-state-metrics"}[5m])))
> 0.01
      For:  15m
      Labels:
        Severity:  critical
      Alert:       KubeStateMetricsWatchErrors
      Annotations:
        Description:  kube-state-metrics is experiencing errors at an elevated rate in watch operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricswatcherrors
        Summary:      kube-state-metrics is experiencing errors in watch operations.
      Expr:           (sum(rate(kube_state_metrics_watch_total{job="kube-state-metrics",result="error"}[5m]))
  /
sum(rate(kube_state_metrics_watch_total{job="kube-state-metrics"}[5m])))
> 0.01
      For:  15m
      Labels:
        Severity:  critical
      Alert:       KubeStateMetricsShardingMismatch
      Annotations:
        Description:  kube-state-metrics pods are running with different --total-shards configuration, some Kubernetes objects may be exposed multiple times or not exposed at all.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricsshardingmismatch
        Summary:      kube-state-metrics sharding is misconfigured.
      Expr:           stdvar (kube_state_metrics_total_shards{job="kube-state-metrics"}) != 0
      For:            15m
      Labels:
        Severity:  critical
      Alert:       KubeStateMetricsShardsMissing
      Annotations:
        Description:  kube-state-metrics shards are missing, some Kubernetes objects are not being exposed.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricsshardsmissing
        Summary:      kube-state-metrics shards are missing.
      Expr:           2^max(kube_state_metrics_total_shards{job="kube-state-metrics"}) - 1
  -
sum( 2 ^ max by (shard_ordinal) (kube_state_metrics_shard_ordinal{job="kube-state-metrics"}) )
!= 0
      For:  15m
      Labels:
        Severity:  critical
Events:            <none>


Name:         prometheus-kube-prometheus-kubelet.rules
Namespace:    default
Labels:       app=kube-prometheus-stack
              app.kubernetes.io/instance=prometheus
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/part-of=kube-prometheus-stack
              app.kubernetes.io/version=45.2.0
              chart=kube-prometheus-stack-45.2.0
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-22T21:15:22Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"kubelet.rules"}:
            .:
            f:name:
            f:rules:
    Manager:         helm
    Operation:       Update
    Time:            2023-02-22T21:15:21Z
  Resource Version:  686
  UID:               5b5d5b70-db1e-4719-b40d-ebb9084d4198
Spec:
  Groups:
    Name:  kubelet.rules
    Rules:
      Expr:  histogram_quantile(0.99, sum(rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) by (cluster, instance, le) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
      Labels:
        Quantile:  0.99
      Record:      node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
      Expr:        histogram_quantile(0.9, sum(rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) by (cluster, instance, le) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
      Labels:
        Quantile:  0.9
      Record:      node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
      Expr:        histogram_quantile(0.5, sum(rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) by (cluster, instance, le) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
      Labels:
        Quantile:  0.5
      Record:      node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
Events:            <none>


Name:         prometheus-kube-prometheus-kubernetes-apps
Namespace:    default
Labels:       app=kube-prometheus-stack
              app.kubernetes.io/instance=prometheus
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/part-of=kube-prometheus-stack
              app.kubernetes.io/version=45.2.0
              chart=kube-prometheus-stack-45.2.0
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-22T21:15:22Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"kubernetes-apps"}:
            .:
            f:name:
            f:rules:
    Manager:         helm
    Operation:       Update
    Time:            2023-02-22T21:15:21Z
  Resource Version:  678
  UID:               066f53cc-370b-4409-8694-360b7e673def
Spec:
  Groups:
    Name:  kubernetes-apps
    Rules:
      Alert:  KubePodCrashLooping
      Annotations:
        Description:  Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is in waiting state (reason: "CrashLoopBackOff").
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepodcrashlooping
        Summary:      Pod is crash looping.
      Expr:           max_over_time(kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff", job="kube-state-metrics", namespace=~".*"}[5m]) >= 1
      For:            15m
      Labels:
        Severity:  warning
      Alert:       KubePodNotReady
      Annotations:
        Description:  Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than 15 minutes.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepodnotready
        Summary:      Pod has been in a non-ready state for more than 15 minutes.
      Expr:           sum by (namespace, pod, cluster) (
  max by(namespace, pod, cluster) (
    kube_pod_status_phase{job="kube-state-metrics", namespace=~".*", phase=~"Pending|Unknown|Failed"}
  ) * on(namespace, pod, cluster) group_left(owner_kind) topk by(namespace, pod, cluster) (
    1, max by(namespace, pod, owner_kind, cluster) (kube_pod_owner{owner_kind!="Job"})
  )
) > 0
      For:  15m
      Labels:
        Severity:  warning
      Alert:       KubeDeploymentGenerationMismatch
      Annotations:
        Description:  Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment }} does not match, this indicates that the Deployment has failed but has not been rolled back.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentgenerationmismatch
        Summary:      Deployment generation mismatch due to possible roll-back
      Expr:           kube_deployment_status_observed_generation{job="kube-state-metrics", namespace=~".*"}
  !=
kube_deployment_metadata_generation{job="kube-state-metrics", namespace=~".*"}
      For:  15m
      Labels:
        Severity:  warning
      Alert:       KubeDeploymentReplicasMismatch
      Annotations:
        Description:  Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not matched the expected number of replicas for longer than 15 minutes.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentreplicasmismatch
        Summary:      Deployment has not matched the expected number of replicas.
      Expr:           (
  kube_deployment_spec_replicas{job="kube-state-metrics", namespace=~".*"}
    >
  kube_deployment_status_replicas_available{job="kube-state-metrics", namespace=~".*"}
) and (
  changes(kube_deployment_status_replicas_updated{job="kube-state-metrics", namespace=~".*"}[10m])
    ==
  0
)
      For:  15m
      Labels:
        Severity:  warning
      Alert:       KubeStatefulSetReplicasMismatch
      Annotations:
        Description:  StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has not matched the expected number of replicas for longer than 15 minutes.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetreplicasmismatch
        Summary:      Deployment has not matched the expected number of replicas.
      Expr:           (
  kube_statefulset_status_replicas_ready{job="kube-state-metrics", namespace=~".*"}
    !=
  kube_statefulset_status_replicas{job="kube-state-metrics", namespace=~".*"}
) and (
  changes(kube_statefulset_status_replicas_updated{job="kube-state-metrics", namespace=~".*"}[10m])
    ==
  0
)
      For:  15m
      Labels:
        Severity:  warning
      Alert:       KubeStatefulSetGenerationMismatch
      Annotations:
        Description:  StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset }} does not match, this indicates that the StatefulSet has failed but has not been rolled back.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetgenerationmismatch
        Summary:      StatefulSet generation mismatch due to possible roll-back
      Expr:           kube_statefulset_status_observed_generation{job="kube-state-metrics", namespace=~".*"}
  !=
kube_statefulset_metadata_generation{job="kube-state-metrics", namespace=~".*"}
      For:  15m
      Labels:
        Severity:  warning
      Alert:       KubeStatefulSetUpdateNotRolledOut
      Annotations:
        Description:  StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update has not been rolled out.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetupdatenotrolledout
        Summary:      StatefulSet update has not been rolled out.
      Expr:           (
  max without (revision) (
    kube_statefulset_status_current_revision{job="kube-state-metrics", namespace=~".*"}
      unless
    kube_statefulset_status_update_revision{job="kube-state-metrics", namespace=~".*"}
  )
    *
  (
    kube_statefulset_replicas{job="kube-state-metrics", namespace=~".*"}
      !=
    kube_statefulset_status_replicas_updated{job="kube-state-metrics", namespace=~".*"}
  )
)  and (
  changes(kube_statefulset_status_replicas_updated{job="kube-state-metrics", namespace=~".*"}[5m])
    ==
  0
)
      For:  15m
      Labels:
        Severity:  warning
      Alert:       KubeDaemonSetRolloutStuck
      Annotations:
        Description:  DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} has not finished or progressed for at least 15 minutes.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetrolloutstuck
        Summary:      DaemonSet rollout is stuck.
      Expr:           (
  (
    kube_daemonset_status_current_number_scheduled{job="kube-state-metrics", namespace=~".*"}
     !=
    kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics", namespace=~".*"}
  ) or (
    kube_daemonset_status_number_misscheduled{job="kube-state-metrics", namespace=~".*"}
     !=
    0
  ) or (
    kube_daemonset_status_updated_number_scheduled{job="kube-state-metrics", namespace=~".*"}
     !=
    kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics", namespace=~".*"}
  ) or (
    kube_daemonset_status_number_available{job="kube-state-metrics", namespace=~".*"}
     !=
    kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics", namespace=~".*"}
  )
) and (
  changes(kube_daemonset_status_updated_number_scheduled{job="kube-state-metrics", namespace=~".*"}[5m])
    ==
  0
)
      For:  15m
      Labels:
        Severity:  warning
      Alert:       KubeContainerWaiting
      Annotations:
        Description:  pod/{{ $labels.pod }} in namespace {{ $labels.namespace }} on container {{ $labels.container}} has been in waiting state for longer than 1 hour.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecontainerwaiting
        Summary:      Pod container waiting longer than 1 hour
      Expr:           sum by (namespace, pod, container, cluster) (kube_pod_container_status_waiting_reason{job="kube-state-metrics", namespace=~".*"}) > 0
      For:            1h
      Labels:
        Severity:  warning
      Alert:       KubeDaemonSetNotScheduled
      Annotations:
        Description:  {{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are not scheduled.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetnotscheduled
        Summary:      DaemonSet pods are not scheduled.
      Expr:           kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics", namespace=~".*"}
  -
kube_daemonset_status_current_number_scheduled{job="kube-state-metrics", namespace=~".*"} > 0
      For:  10m
      Labels:
        Severity:  warning
      Alert:       KubeDaemonSetMisScheduled
      Annotations:
        Description:  {{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are running where they are not supposed to run.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetmisscheduled
        Summary:      DaemonSet pods are misscheduled.
      Expr:           kube_daemonset_status_number_misscheduled{job="kube-state-metrics", namespace=~".*"} > 0
      For:            15m
      Labels:
        Severity:  warning
      Alert:       KubeJobNotCompleted
      Annotations:
        Description:  Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more than {{ "43200" | humanizeDuration }} to complete.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubejobnotcompleted
        Summary:      Job did not complete in time
      Expr:           time() - max by(namespace, job_name, cluster) (kube_job_status_start_time{job="kube-state-metrics", namespace=~".*"}
  and
kube_job_status_active{job="kube-state-metrics", namespace=~".*"} > 0) > 43200
      Labels:
        Severity:  warning
      Alert:       KubeJobFailed
      Annotations:
        Description:  Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete. Removing failed job after investigation should clear this alert.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubejobfailed
        Summary:      Job failed to complete.
      Expr:           kube_job_failed{job="kube-state-metrics", namespace=~".*"}  > 0
      For:            15m
      Labels:
        Severity:  warning
      Alert:       KubeHpaReplicasMismatch
      Annotations:
        Description:  HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }} has not matched the desired number of replicas for longer than 15 minutes.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubehpareplicasmismatch
        Summary:      HPA has not matched desired number of replicas.
      Expr:           (kube_horizontalpodautoscaler_status_desired_replicas{job="kube-state-metrics", namespace=~".*"}
  !=
kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics", namespace=~".*"})
  and
(kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics", namespace=~".*"}
  >
kube_horizontalpodautoscaler_spec_min_replicas{job="kube-state-metrics", namespace=~".*"})
  and
(kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics", namespace=~".*"}
  <
kube_horizontalpodautoscaler_spec_max_replicas{job="kube-state-metrics", namespace=~".*"})
  and
changes(kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics", namespace=~".*"}[15m]) == 0
      For:  15m
      Labels:
        Severity:  warning
      Alert:       KubeHpaMaxedOut
      Annotations:
        Description:  HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }} has been running at max replicas for longer than 15 minutes.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubehpamaxedout
        Summary:      HPA is running at max replicas
      Expr:           kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics", namespace=~".*"}
  ==
kube_horizontalpodautoscaler_spec_max_replicas{job="kube-state-metrics", namespace=~".*"}
      For:  15m
      Labels:
        Severity:  warning
Events:            <none>


Name:         prometheus-kube-prometheus-kubernetes-resources
Namespace:    default
Labels:       app=kube-prometheus-stack
              app.kubernetes.io/instance=prometheus
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/part-of=kube-prometheus-stack
              app.kubernetes.io/version=45.2.0
              chart=kube-prometheus-stack-45.2.0
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-22T21:15:22Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"kubernetes-resources"}:
            .:
            f:name:
            f:rules:
    Manager:         helm
    Operation:       Update
    Time:            2023-02-22T21:15:21Z
  Resource Version:  673
  UID:               6b2d01b1-a65d-4552-beb2-985540278ae5
Spec:
  Groups:
    Name:  kubernetes-resources
    Rules:
      Alert:  KubeCPUOvercommit
      Annotations:
        Description:  Cluster has overcommitted CPU resource requests for Pods by {{ $value }} CPU shares and cannot tolerate node failure.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecpuovercommit
        Summary:      Cluster has overcommitted CPU resource requests.
      Expr:           sum(namespace_cpu:kube_pod_container_resource_requests:sum{}) - (sum(kube_node_status_allocatable{resource="cpu"}) - max(kube_node_status_allocatable{resource="cpu"})) > 0
and
(sum(kube_node_status_allocatable{resource="cpu"}) - max(kube_node_status_allocatable{resource="cpu"})) > 0
      For:  10m
      Labels:
        Severity:  warning
      Alert:       KubeMemoryOvercommit
      Annotations:
        Description:  Cluster has overcommitted memory resource requests for Pods by {{ $value | humanize }} bytes and cannot tolerate node failure.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubememoryovercommit
        Summary:      Cluster has overcommitted memory resource requests.
      Expr:           sum(namespace_memory:kube_pod_container_resource_requests:sum{}) - (sum(kube_node_status_allocatable{resource="memory"}) - max(kube_node_status_allocatable{resource="memory"})) > 0
and
(sum(kube_node_status_allocatable{resource="memory"}) - max(kube_node_status_allocatable{resource="memory"})) > 0
      For:  10m
      Labels:
        Severity:  warning
      Alert:       KubeCPUQuotaOvercommit
      Annotations:
        Description:  Cluster has overcommitted CPU resource requests for Namespaces.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecpuquotaovercommit
        Summary:      Cluster has overcommitted CPU resource requests.
      Expr:           sum(min without(resource) (kube_resourcequota{job="kube-state-metrics", type="hard", resource=~"(cpu|requests.cpu)"}))
  /
sum(kube_node_status_allocatable{resource="cpu", job="kube-state-metrics"})
  > 1.5
      For:  5m
      Labels:
        Severity:  warning
      Alert:       KubeMemoryQuotaOvercommit
      Annotations:
        Description:  Cluster has overcommitted memory resource requests for Namespaces.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubememoryquotaovercommit
        Summary:      Cluster has overcommitted memory resource requests.
      Expr:           sum(min without(resource) (kube_resourcequota{job="kube-state-metrics", type="hard", resource=~"(memory|requests.memory)"}))
  /
sum(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"})
  > 1.5
      For:  5m
      Labels:
        Severity:  warning
      Alert:       KubeQuotaAlmostFull
      Annotations:
        Description:  Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotaalmostfull
        Summary:      Namespace quota is going to be full.
      Expr:           kube_resourcequota{job="kube-state-metrics", type="used"}
  / ignoring(instance, job, type)
(kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
  > 0.9 < 1
      For:  15m
      Labels:
        Severity:  info
      Alert:       KubeQuotaFullyUsed
      Annotations:
        Description:  Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotafullyused
        Summary:      Namespace quota is fully used.
      Expr:           kube_resourcequota{job="kube-state-metrics", type="used"}
  / ignoring(instance, job, type)
(kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
  == 1
      For:  15m
      Labels:
        Severity:  info
      Alert:       KubeQuotaExceeded
      Annotations:
        Description:  Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotaexceeded
        Summary:      Namespace quota has exceeded the limits.
      Expr:           kube_resourcequota{job="kube-state-metrics", type="used"}
  / ignoring(instance, job, type)
(kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
  > 1
      For:  15m
      Labels:
        Severity:  warning
      Alert:       CPUThrottlingHigh
      Annotations:
        Description:  {{ $value | humanizePercentage }} throttling of CPU in namespace {{ $labels.namespace }} for container {{ $labels.container }} in pod {{ $labels.pod }}.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/cputhrottlinghigh
        Summary:      Processes experience elevated CPU throttling.
      Expr:           sum(increase(container_cpu_cfs_throttled_periods_total{container!="", }[5m])) by (container, pod, namespace)
  /
sum(increase(container_cpu_cfs_periods_total{}[5m])) by (container, pod, namespace)
  > ( 25 / 100 )
      For:  15m
      Labels:
        Severity:  info
Events:            <none>


Name:         prometheus-kube-prometheus-kubernetes-storage
Namespace:    default
Labels:       app=kube-prometheus-stack
              app.kubernetes.io/instance=prometheus
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/part-of=kube-prometheus-stack
              app.kubernetes.io/version=45.2.0
              chart=kube-prometheus-stack-45.2.0
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-22T21:15:22Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"kubernetes-storage"}:
            .:
            f:name:
            f:rules:
    Manager:         helm
    Operation:       Update
    Time:            2023-02-22T21:15:21Z
  Resource Version:  683
  UID:               5d0344ef-31eb-4127-b463-9b3eb18b015d
Spec:
  Groups:
    Name:  kubernetes-storage
    Rules:
      Alert:  KubePersistentVolumeFillingUp
      Annotations:
        Description:  The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage }} free.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumefillingup
        Summary:      PersistentVolume is filling up.
      Expr:           (
  kubelet_volume_stats_available_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
    /
  kubelet_volume_stats_capacity_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
) < 0.03
and
kubelet_volume_stats_used_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"} > 0
unless on(namespace, persistentvolumeclaim)
kube_persistentvolumeclaim_access_mode{ access_mode="ReadOnlyMany"} == 1
unless on(namespace, persistentvolumeclaim)
kube_persistentvolumeclaim_labels{label_excluded_from_alerts="true"} == 1
      For:  1m
      Labels:
        Severity:  critical
      Alert:       KubePersistentVolumeFillingUp
      Annotations:
        Description:  Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is expected to fill up within four days. Currently {{ $value | humanizePercentage }} is available.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumefillingup
        Summary:      PersistentVolume is filling up.
      Expr:           (
  kubelet_volume_stats_available_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
    /
  kubelet_volume_stats_capacity_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
) < 0.15
and
kubelet_volume_stats_used_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"} > 0
and
predict_linear(kubelet_volume_stats_available_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"}[6h], 4 * 24 * 3600) < 0
unless on(namespace, persistentvolumeclaim)
kube_persistentvolumeclaim_access_mode{ access_mode="ReadOnlyMany"} == 1
unless on(namespace, persistentvolumeclaim)
kube_persistentvolumeclaim_labels{label_excluded_from_alerts="true"} == 1
      For:  1h
      Labels:
        Severity:  warning
      Alert:       KubePersistentVolumeInodesFillingUp
      Annotations:
        Description:  The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} only has {{ $value | humanizePercentage }} free inodes.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumeinodesfillingup
        Summary:      PersistentVolumeInodes are filling up.
      Expr:           (
  kubelet_volume_stats_inodes_free{job="kubelet", namespace=~".*", metrics_path="/metrics"}
    /
  kubelet_volume_stats_inodes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
) < 0.03
and
kubelet_volume_stats_inodes_used{job="kubelet", namespace=~".*", metrics_path="/metrics"} > 0
unless on(namespace, persistentvolumeclaim)
kube_persistentvolumeclaim_access_mode{ access_mode="ReadOnlyMany"} == 1
unless on(namespace, persistentvolumeclaim)
kube_persistentvolumeclaim_labels{label_excluded_from_alerts="true"} == 1
      For:  1m
      Labels:
        Severity:  critical
      Alert:       KubePersistentVolumeInodesFillingUp
      Annotations:
        Description:  Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is expected to run out of inodes within four days. Currently {{ $value | humanizePercentage }} of its inodes are free.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumeinodesfillingup
        Summary:      PersistentVolumeInodes are filling up.
      Expr:           (
  kubelet_volume_stats_inodes_free{job="kubelet", namespace=~".*", metrics_path="/metrics"}
    /
  kubelet_volume_stats_inodes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
) < 0.15
and
kubelet_volume_stats_inodes_used{job="kubelet", namespace=~".*", metrics_path="/metrics"} > 0
and
predict_linear(kubelet_volume_stats_inodes_free{job="kubelet", namespace=~".*", metrics_path="/metrics"}[6h], 4 * 24 * 3600) < 0
unless on(namespace, persistentvolumeclaim)
kube_persistentvolumeclaim_access_mode{ access_mode="ReadOnlyMany"} == 1
unless on(namespace, persistentvolumeclaim)
kube_persistentvolumeclaim_labels{label_excluded_from_alerts="true"} == 1
      For:  1h
      Labels:
        Severity:  warning
      Alert:       KubePersistentVolumeErrors
      Annotations:
        Description:  The persistent volume {{ $labels.persistentvolume }} has status {{ $labels.phase }}.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumeerrors
        Summary:      PersistentVolume is having issues with provisioning.
      Expr:           kube_persistentvolume_status_phase{phase=~"Failed|Pending",job="kube-state-metrics"} > 0
      For:            5m
      Labels:
        Severity:  critical
Events:            <none>


Name:         prometheus-kube-prometheus-kubernetes-system
Namespace:    default
Labels:       app=kube-prometheus-stack
              app.kubernetes.io/instance=prometheus
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/part-of=kube-prometheus-stack
              app.kubernetes.io/version=45.2.0
              chart=kube-prometheus-stack-45.2.0
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-22T21:15:22Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"kubernetes-system"}:
            .:
            f:name:
            f:rules:
    Manager:         helm
    Operation:       Update
    Time:            2023-02-22T21:15:21Z
  Resource Version:  682
  UID:               69a99def-fdb4-4e78-b364-8b6de73e46bd
Spec:
  Groups:
    Name:  kubernetes-system
    Rules:
      Alert:  KubeVersionMismatch
      Annotations:
        Description:  There are {{ $value }} different semantic versions of Kubernetes components running.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeversionmismatch
        Summary:      Different semantic versions of Kubernetes components running.
      Expr:           count by (cluster) (count by (git_version, cluster) (label_replace(kubernetes_build_info{job!~"kube-dns|coredns"},"git_version","$1","git_version","(v[0-9]*.[0-9]*).*"))) > 1
      For:            15m
      Labels:
        Severity:  warning
      Alert:       KubeClientErrors
      Annotations:
        Description:  Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing {{ $value | humanizePercentage }} errors.'
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeclienterrors
        Summary:      Kubernetes API server client is experiencing errors.
      Expr:           (sum(rate(rest_client_requests_total{code=~"5.."}[5m])) by (cluster, instance, job, namespace)
  /
sum(rate(rest_client_requests_total[5m])) by (cluster, instance, job, namespace))
> 0.01
      For:  15m
      Labels:
        Severity:  warning
Events:            <none>


Name:         prometheus-kube-prometheus-kubernetes-system-apiserver
Namespace:    default
Labels:       app=kube-prometheus-stack
              app.kubernetes.io/instance=prometheus
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/part-of=kube-prometheus-stack
              app.kubernetes.io/version=45.2.0
              chart=kube-prometheus-stack-45.2.0
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-22T21:15:22Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"kubernetes-system-apiserver"}:
            .:
            f:name:
            f:rules:
    Manager:         helm
    Operation:       Update
    Time:            2023-02-22T21:15:21Z
  Resource Version:  693
  UID:               1df6fe60-69a5-4317-bdf4-708f6f9d4515
Spec:
  Groups:
    Name:  kubernetes-system-apiserver
    Rules:
      Alert:  KubeClientCertificateExpiration
      Annotations:
        Description:  A client certificate used to authenticate to kubernetes apiserver is expiring in less than 7.0 days.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeclientcertificateexpiration
        Summary:      Client certificate is about to expire.
      Expr:           apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and on(job) histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 604800
      For:            5m
      Labels:
        Severity:  warning
      Alert:       KubeClientCertificateExpiration
      Annotations:
        Description:  A client certificate used to authenticate to kubernetes apiserver is expiring in less than 24.0 hours.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeclientcertificateexpiration
        Summary:      Client certificate is about to expire.
      Expr:           apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and on(job) histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 86400
      For:            5m
      Labels:
        Severity:  critical
      Alert:       KubeAggregatedAPIErrors
      Annotations:
        Description:  Kubernetes aggregated API {{ $labels.name }}/{{ $labels.namespace }} has reported errors. It has appeared unavailable {{ $value | humanize }} times averaged over the past 10m.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeaggregatedapierrors
        Summary:      Kubernetes aggregated API has reported errors.
      Expr:           sum by(name, namespace, cluster)(increase(aggregator_unavailable_apiservice_total[10m])) > 4
      Labels:
        Severity:  warning
      Alert:       KubeAggregatedAPIDown
      Annotations:
        Description:  Kubernetes aggregated API {{ $labels.name }}/{{ $labels.namespace }} has been only {{ $value | humanize }}% available over the last 10m.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeaggregatedapidown
        Summary:      Kubernetes aggregated API is down.
      Expr:           (1 - max by(name, namespace, cluster)(avg_over_time(aggregator_unavailable_apiservice[10m]))) * 100 < 85
      For:            5m
      Labels:
        Severity:  warning
      Alert:       KubeAPIDown
      Annotations:
        Description:  KubeAPI has disappeared from Prometheus target discovery.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapidown
        Summary:      Target disappeared from Prometheus target discovery.
      Expr:           absent(up{job="apiserver"} == 1)
      For:            15m
      Labels:
        Severity:  critical
      Alert:       KubeAPITerminatedRequests
      Annotations:
        Description:  The kubernetes apiserver has terminated {{ $value | humanizePercentage }} of its incoming requests.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapiterminatedrequests
        Summary:      The kubernetes apiserver has terminated {{ $value | humanizePercentage }} of its incoming requests.
      Expr:           sum(rate(apiserver_request_terminations_total{job="apiserver"}[10m]))  / (  sum(rate(apiserver_request_total{job="apiserver"}[10m])) + sum(rate(apiserver_request_terminations_total{job="apiserver"}[10m])) ) > 0.20
      For:            5m
      Labels:
        Severity:  warning
Events:            <none>


Name:         prometheus-kube-prometheus-kubernetes-system-controller-manager
Namespace:    default
Labels:       app=kube-prometheus-stack
              app.kubernetes.io/instance=prometheus
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/part-of=kube-prometheus-stack
              app.kubernetes.io/version=45.2.0
              chart=kube-prometheus-stack-45.2.0
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-22T21:15:22Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"kubernetes-system-controller-manager"}:
            .:
            f:name:
            f:rules:
    Manager:         helm
    Operation:       Update
    Time:            2023-02-22T21:15:21Z
  Resource Version:  674
  UID:               8ba87ae9-c717-47e2-bdba-e83a755f1c11
Spec:
  Groups:
    Name:  kubernetes-system-controller-manager
    Rules:
      Alert:  KubeControllerManagerDown
      Annotations:
        Description:  KubeControllerManager has disappeared from Prometheus target discovery.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecontrollermanagerdown
        Summary:      Target disappeared from Prometheus target discovery.
      Expr:           absent(up{job="kube-controller-manager"} == 1)
      For:            15m
      Labels:
        Severity:  critical
Events:            <none>


Name:         prometheus-kube-prometheus-kubernetes-system-kube-proxy
Namespace:    default
Labels:       app=kube-prometheus-stack
              app.kubernetes.io/instance=prometheus
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/part-of=kube-prometheus-stack
              app.kubernetes.io/version=45.2.0
              chart=kube-prometheus-stack-45.2.0
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-22T21:15:22Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"kubernetes-system-kube-proxy"}:
            .:
            f:name:
            f:rules:
    Manager:         helm
    Operation:       Update
    Time:            2023-02-22T21:15:21Z
  Resource Version:  676
  UID:               1fe989b8-ef92-4620-8fbe-f2f20b79bf19
Spec:
  Groups:
    Name:  kubernetes-system-kube-proxy
    Rules:
      Alert:  KubeProxyDown
      Annotations:
        Description:  KubeProxy has disappeared from Prometheus target discovery.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeproxydown
        Summary:      Target disappeared from Prometheus target discovery.
      Expr:           absent(up{job="kube-proxy"} == 1)
      For:            15m
      Labels:
        Severity:  critical
Events:            <none>


Name:         prometheus-kube-prometheus-kubernetes-system-kubelet
Namespace:    default
Labels:       app=kube-prometheus-stack
              app.kubernetes.io/instance=prometheus
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/part-of=kube-prometheus-stack
              app.kubernetes.io/version=45.2.0
              chart=kube-prometheus-stack-45.2.0
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-22T21:15:22Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"kubernetes-system-kubelet"}:
            .:
            f:name:
            f:rules:
    Manager:         helm
    Operation:       Update
    Time:            2023-02-22T21:15:21Z
  Resource Version:  698
  UID:               680d30fe-d92b-4fd7-8213-738453fd5283
Spec:
  Groups:
    Name:  kubernetes-system-kubelet
    Rules:
      Alert:  KubeNodeNotReady
      Annotations:
        Description:  {{ $labels.node }} has been unready for more than 15 minutes.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodenotready
        Summary:      Node is not ready.
      Expr:           kube_node_status_condition{job="kube-state-metrics",condition="Ready",status="true"} == 0
      For:            15m
      Labels:
        Severity:  warning
      Alert:       KubeNodeUnreachable
      Annotations:
        Description:  {{ $labels.node }} is unreachable and some workloads may be rescheduled.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodeunreachable
        Summary:      Node is unreachable.
      Expr:           (kube_node_spec_taint{job="kube-state-metrics",key="node.kubernetes.io/unreachable",effect="NoSchedule"} unless ignoring(key,value) kube_node_spec_taint{job="kube-state-metrics",key=~"ToBeDeletedByClusterAutoscaler|cloud.google.com/impending-node-termination|aws-node-termination-handler/spot-itn"}) == 1
      For:            15m
      Labels:
        Severity:  warning
      Alert:       KubeletTooManyPods
      Annotations:
        Description:  Kubelet '{{ $labels.node }}' is running at {{ $value | humanizePercentage }} of its Pod capacity.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubelettoomanypods
        Summary:      Kubelet is running at capacity.
      Expr:           count by(cluster, node) (
  (kube_pod_status_phase{job="kube-state-metrics",phase="Running"} == 1) * on(instance,pod,namespace,cluster) group_left(node) topk by(instance,pod,namespace,cluster) (1, kube_pod_info{job="kube-state-metrics"})
)
/
max by(cluster, node) (
  kube_node_status_capacity{job="kube-state-metrics",resource="pods"} != 1
) > 0.95
      For:  15m
      Labels:
        Severity:  info
      Alert:       KubeNodeReadinessFlapping
      Annotations:
        Description:  The readiness status of node {{ $labels.node }} has changed {{ $value }} times in the last 15 minutes.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodereadinessflapping
        Summary:      Node readiness status is flapping.
      Expr:           sum(changes(kube_node_status_condition{status="true",condition="Ready"}[15m])) by (cluster, node) > 2
      For:            15m
      Labels:
        Severity:  warning
      Alert:       KubeletPlegDurationHigh
      Annotations:
        Description:  The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of {{ $value }} seconds on node {{ $labels.node }}.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletplegdurationhigh
        Summary:      Kubelet Pod Lifecycle Event Generator is taking too long to relist.
      Expr:           node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile{quantile="0.99"} >= 10
      For:            5m
      Labels:
        Severity:  warning
      Alert:       KubeletPodStartUpLatencyHigh
      Annotations:
        Description:  Kubelet Pod startup 99th percentile latency is {{ $value }} seconds on node {{ $labels.node }}.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletpodstartuplatencyhigh
        Summary:      Kubelet Pod startup latency is too high.
      Expr:           histogram_quantile(0.99, sum(rate(kubelet_pod_worker_duration_seconds_bucket{job="kubelet", metrics_path="/metrics"}[5m])) by (cluster, instance, le)) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"} > 60
      For:            15m
      Labels:
        Severity:  warning
      Alert:       KubeletClientCertificateExpiration
      Annotations:
        Description:  Client certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }}.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletclientcertificateexpiration
        Summary:      Kubelet client certificate is about to expire.
      Expr:           kubelet_certificate_manager_client_ttl_seconds < 604800
      Labels:
        Severity:  warning
      Alert:       KubeletClientCertificateExpiration
      Annotations:
        Description:  Client certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }}.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletclientcertificateexpiration
        Summary:      Kubelet client certificate is about to expire.
      Expr:           kubelet_certificate_manager_client_ttl_seconds < 86400
      Labels:
        Severity:  critical
      Alert:       KubeletServerCertificateExpiration
      Annotations:
        Description:  Server certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }}.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletservercertificateexpiration
        Summary:      Kubelet server certificate is about to expire.
      Expr:           kubelet_certificate_manager_server_ttl_seconds < 604800
      Labels:
        Severity:  warning
      Alert:       KubeletServerCertificateExpiration
      Annotations:
        Description:  Server certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }}.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletservercertificateexpiration
        Summary:      Kubelet server certificate is about to expire.
      Expr:           kubelet_certificate_manager_server_ttl_seconds < 86400
      Labels:
        Severity:  critical
      Alert:       KubeletClientCertificateRenewalErrors
      Annotations:
        Description:  Kubelet on node {{ $labels.node }} has failed to renew its client certificate ({{ $value | humanize }} errors in the last 5 minutes).
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletclientcertificaterenewalerrors
        Summary:      Kubelet has failed to renew its client certificate.
      Expr:           increase(kubelet_certificate_manager_client_expiration_renew_errors[5m]) > 0
      For:            15m
      Labels:
        Severity:  warning
      Alert:       KubeletServerCertificateRenewalErrors
      Annotations:
        Description:  Kubelet on node {{ $labels.node }} has failed to renew its server certificate ({{ $value | humanize }} errors in the last 5 minutes).
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletservercertificaterenewalerrors
        Summary:      Kubelet has failed to renew its server certificate.
      Expr:           increase(kubelet_server_expiration_renew_errors[5m]) > 0
      For:            15m
      Labels:
        Severity:  warning
      Alert:       KubeletDown
      Annotations:
        Description:  Kubelet has disappeared from Prometheus target discovery.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletdown
        Summary:      Target disappeared from Prometheus target discovery.
      Expr:           absent(up{job="kubelet", metrics_path="/metrics"} == 1)
      For:            15m
      Labels:
        Severity:  critical
Events:            <none>


Name:         prometheus-kube-prometheus-kubernetes-system-scheduler
Namespace:    default
Labels:       app=kube-prometheus-stack
              app.kubernetes.io/instance=prometheus
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/part-of=kube-prometheus-stack
              app.kubernetes.io/version=45.2.0
              chart=kube-prometheus-stack-45.2.0
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-22T21:15:22Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"kubernetes-system-scheduler"}:
            .:
            f:name:
            f:rules:
    Manager:         helm
    Operation:       Update
    Time:            2023-02-22T21:15:21Z
  Resource Version:  670
  UID:               594559b3-6850-4fc3-a650-c95c6ab982ab
Spec:
  Groups:
    Name:  kubernetes-system-scheduler
    Rules:
      Alert:  KubeSchedulerDown
      Annotations:
        Description:  KubeScheduler has disappeared from Prometheus target discovery.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeschedulerdown
        Summary:      Target disappeared from Prometheus target discovery.
      Expr:           absent(up{job="kube-scheduler"} == 1)
      For:            15m
      Labels:
        Severity:  critical
Events:            <none>


Name:         prometheus-kube-prometheus-node-exporter
Namespace:    default
Labels:       app=kube-prometheus-stack
              app.kubernetes.io/instance=prometheus
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/part-of=kube-prometheus-stack
              app.kubernetes.io/version=45.2.0
              chart=kube-prometheus-stack-45.2.0
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-22T21:15:22Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"node-exporter"}:
            .:
            f:name:
            f:rules:
    Manager:         helm
    Operation:       Update
    Time:            2023-02-22T21:15:21Z
  Resource Version:  685
  UID:               6d6d0ba8-144a-4670-b7cf-93945fe5311c
Spec:
  Groups:
    Name:  node-exporter
    Rules:
      Alert:  NodeFilesystemSpaceFillingUp
      Annotations:
        Description:  Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left and is filling up.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemspacefillingup
        Summary:      Filesystem is predicted to run out of space within the next 24 hours.
      Expr:           (
  node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 15
and
  predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""}[6h], 24*60*60) < 0
and
  node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
)
      For:  1h
      Labels:
        Severity:  warning
      Alert:       NodeFilesystemSpaceFillingUp
      Annotations:
        Description:  Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left and is filling up fast.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemspacefillingup
        Summary:      Filesystem is predicted to run out of space within the next 4 hours.
      Expr:           (
  node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 10
and
  predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""}[6h], 4*60*60) < 0
and
  node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
)
      For:  1h
      Labels:
        Severity:  critical
      Alert:       NodeFilesystemAlmostOutOfSpace
      Annotations:
        Description:  Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutofspace
        Summary:      Filesystem has less than 5% space left.
      Expr:           (
  node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 5
and
  node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
)
      For:  30m
      Labels:
        Severity:  warning
      Alert:       NodeFilesystemAlmostOutOfSpace
      Annotations:
        Description:  Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutofspace
        Summary:      Filesystem has less than 3% space left.
      Expr:           (
  node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 3
and
  node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
)
      For:  30m
      Labels:
        Severity:  critical
      Alert:       NodeFilesystemFilesFillingUp
      Annotations:
        Description:  Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left and is filling up.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemfilesfillingup
        Summary:      Filesystem is predicted to run out of inodes within the next 24 hours.
      Expr:           (
  node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 40
and
  predict_linear(node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""}[6h], 24*60*60) < 0
and
  node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
)
      For:  1h
      Labels:
        Severity:  warning
      Alert:       NodeFilesystemFilesFillingUp
      Annotations:
        Description:  Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left and is filling up fast.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemfilesfillingup
        Summary:      Filesystem is predicted to run out of inodes within the next 4 hours.
      Expr:           (
  node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 20
and
  predict_linear(node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""}[6h], 4*60*60) < 0
and
  node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
)
      For:  1h
      Labels:
        Severity:  critical
      Alert:       NodeFilesystemAlmostOutOfFiles
      Annotations:
        Description:  Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutoffiles
        Summary:      Filesystem has less than 5% inodes left.
      Expr:           (
  node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 5
and
  node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
)
      For:  1h
      Labels:
        Severity:  warning
      Alert:       NodeFilesystemAlmostOutOfFiles
      Annotations:
        Description:  Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutoffiles
        Summary:      Filesystem has less than 3% inodes left.
      Expr:           (
  node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 3
and
  node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
)
      For:  1h
      Labels:
        Severity:  critical
      Alert:       NodeNetworkReceiveErrs
      Annotations:
        Description:  {{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf "%.0f" $value }} receive errors in the last two minutes.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/node/nodenetworkreceiveerrs
        Summary:      Network interface is reporting many receive errors.
      Expr:           rate(node_network_receive_errs_total[2m]) / rate(node_network_receive_packets_total[2m]) > 0.01
      For:            1h
      Labels:
        Severity:  warning
      Alert:       NodeNetworkTransmitErrs
      Annotations:
        Description:  {{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf "%.0f" $value }} transmit errors in the last two minutes.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/node/nodenetworktransmiterrs
        Summary:      Network interface is reporting many transmit errors.
      Expr:           rate(node_network_transmit_errs_total[2m]) / rate(node_network_transmit_packets_total[2m]) > 0.01
      For:            1h
      Labels:
        Severity:  warning
      Alert:       NodeHighNumberConntrackEntriesUsed
      Annotations:
        Description:  {{ $value | humanizePercentage }} of conntrack entries are used.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/node/nodehighnumberconntrackentriesused
        Summary:      Number of conntrack are getting close to the limit.
      Expr:           (node_nf_conntrack_entries / node_nf_conntrack_entries_limit) > 0.75
      Labels:
        Severity:  warning
      Alert:       NodeTextFileCollectorScrapeError
      Annotations:
        Description:  Node Exporter text file collector failed to scrape.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/node/nodetextfilecollectorscrapeerror
        Summary:      Node Exporter text file collector failed to scrape.
      Expr:           node_textfile_scrape_error{job="node-exporter"} == 1
      Labels:
        Severity:  warning
      Alert:       NodeClockSkewDetected
      Annotations:
        Description:  Clock on {{ $labels.instance }} is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/node/nodeclockskewdetected
        Summary:      Clock skew detected.
      Expr:           (
  node_timex_offset_seconds{job="node-exporter"} > 0.05
and
  deriv(node_timex_offset_seconds{job="node-exporter"}[5m]) >= 0
)
or
(
  node_timex_offset_seconds{job="node-exporter"} < -0.05
and
  deriv(node_timex_offset_seconds{job="node-exporter"}[5m]) <= 0
)
      For:  10m
      Labels:
        Severity:  warning
      Alert:       NodeClockNotSynchronising
      Annotations:
        Description:  Clock on {{ $labels.instance }} is not synchronising. Ensure NTP is configured on this host.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/node/nodeclocknotsynchronising
        Summary:      Clock not synchronising.
      Expr:           min_over_time(node_timex_sync_status{job="node-exporter"}[5m]) == 0
and
node_timex_maxerror_seconds{job="node-exporter"} >= 16
      For:  10m
      Labels:
        Severity:  warning
      Alert:       NodeRAIDDegraded
      Annotations:
        Description:  RAID array '{{ $labels.device }}' on {{ $labels.instance }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/node/noderaiddegraded
        Summary:      RAID Array is degraded
      Expr:           node_md_disks_required{job="node-exporter",device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"} - ignoring (state) (node_md_disks{state="active",job="node-exporter",device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"}) > 0
      For:            15m
      Labels:
        Severity:  critical
      Alert:       NodeRAIDDiskFailure
      Annotations:
        Description:  At least one device in RAID array on {{ $labels.instance }} failed. Array '{{ $labels.device }}' needs attention and possibly a disk swap.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/node/noderaiddiskfailure
        Summary:      Failed device in RAID array
      Expr:           node_md_disks{state="failed",job="node-exporter",device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"} > 0
      Labels:
        Severity:  warning
      Alert:       NodeFileDescriptorLimit
      Annotations:
        Description:  File descriptors limit at {{ $labels.instance }} is currently at {{ printf "%.2f" $value }}%.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/node/nodefiledescriptorlimit
        Summary:      Kernel is predicted to exhaust file descriptors limit soon.
      Expr:           (
  node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} > 70
)
      For:  15m
      Labels:
        Severity:  warning
      Alert:       NodeFileDescriptorLimit
      Annotations:
        Description:  File descriptors limit at {{ $labels.instance }} is currently at {{ printf "%.2f" $value }}%.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/node/nodefiledescriptorlimit
        Summary:      Kernel is predicted to exhaust file descriptors limit soon.
      Expr:           (
  node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} > 90
)
      For:  15m
      Labels:
        Severity:  critical
Events:            <none>


Name:         prometheus-kube-prometheus-node-exporter.rules
Namespace:    default
Labels:       app=kube-prometheus-stack
              app.kubernetes.io/instance=prometheus
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/part-of=kube-prometheus-stack
              app.kubernetes.io/version=45.2.0
              chart=kube-prometheus-stack-45.2.0
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-22T21:15:22Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"node-exporter.rules"}:
            .:
            f:name:
            f:rules:
    Manager:         helm
    Operation:       Update
    Time:            2023-02-22T21:15:21Z
  Resource Version:  692
  UID:               0bd5b7e8-900d-4bb6-8b5e-3b8f263c2b91
Spec:
  Groups:
    Name:  node-exporter.rules
    Rules:
      Expr:  count without (cpu, mode) (
  node_cpu_seconds_total{job="node-exporter",mode="idle"}
)
      Record:  instance:node_num_cpu:sum
      Expr:    1 - avg without (cpu) (
  sum without (mode) (rate(node_cpu_seconds_total{job="node-exporter", mode=~"idle|iowait|steal"}[5m]))
)
      Record:  instance:node_cpu_utilisation:rate5m
      Expr:    (
  node_load1{job="node-exporter"}
/
  instance:node_num_cpu:sum{job="node-exporter"}
)
      Record:  instance:node_load1_per_cpu:ratio
      Expr:    1 - (
  (
    node_memory_MemAvailable_bytes{job="node-exporter"}
    or
    (
      node_memory_Buffers_bytes{job="node-exporter"}
      +
      node_memory_Cached_bytes{job="node-exporter"}
      +
      node_memory_MemFree_bytes{job="node-exporter"}
      +
      node_memory_Slab_bytes{job="node-exporter"}
    )
  )
/
  node_memory_MemTotal_bytes{job="node-exporter"}
)
      Record:  instance:node_memory_utilisation:ratio
      Expr:    rate(node_vmstat_pgmajfault{job="node-exporter"}[5m])
      Record:  instance:node_vmstat_pgmajfault:rate5m
      Expr:    rate(node_disk_io_time_seconds_total{job="node-exporter", device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"}[5m])
      Record:  instance_device:node_disk_io_time_seconds:rate5m
      Expr:    rate(node_disk_io_time_weighted_seconds_total{job="node-exporter", device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"}[5m])
      Record:  instance_device:node_disk_io_time_weighted_seconds:rate5m
      Expr:    sum without (device) (
  rate(node_network_receive_bytes_total{job="node-exporter", device!="lo"}[5m])
)
      Record:  instance:node_network_receive_bytes_excluding_lo:rate5m
      Expr:    sum without (device) (
  rate(node_network_transmit_bytes_total{job="node-exporter", device!="lo"}[5m])
)
      Record:  instance:node_network_transmit_bytes_excluding_lo:rate5m
      Expr:    sum without (device) (
  rate(node_network_receive_drop_total{job="node-exporter", device!="lo"}[5m])
)
      Record:  instance:node_network_receive_drop_excluding_lo:rate5m
      Expr:    sum without (device) (
  rate(node_network_transmit_drop_total{job="node-exporter", device!="lo"}[5m])
)
      Record:  instance:node_network_transmit_drop_excluding_lo:rate5m
Events:        <none>


Name:         prometheus-kube-prometheus-node-network
Namespace:    default
Labels:       app=kube-prometheus-stack
              app.kubernetes.io/instance=prometheus
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/part-of=kube-prometheus-stack
              app.kubernetes.io/version=45.2.0
              chart=kube-prometheus-stack-45.2.0
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-22T21:15:22Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"node-network"}:
            .:
            f:name:
            f:rules:
    Manager:         helm
    Operation:       Update
    Time:            2023-02-22T21:15:21Z
  Resource Version:  681
  UID:               da5faf98-3457-49ef-bb59-e170275d08fb
Spec:
  Groups:
    Name:  node-network
    Rules:
      Alert:  NodeNetworkInterfaceFlapping
      Annotations:
        Description:  Network interface "{{ $labels.device }}" changing its up status often on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/general/nodenetworkinterfaceflapping
        Summary:      Network interface is often changing its status
      Expr:           changes(node_network_up{job="node-exporter",device!~"veth.+"}[2m]) > 2
      For:            2m
      Labels:
        Severity:  warning
Events:            <none>


Name:         prometheus-kube-prometheus-node.rules
Namespace:    default
Labels:       app=kube-prometheus-stack
              app.kubernetes.io/instance=prometheus
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/part-of=kube-prometheus-stack
              app.kubernetes.io/version=45.2.0
              chart=kube-prometheus-stack-45.2.0
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-22T21:15:22Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"node.rules"}:
            .:
            f:name:
            f:rules:
    Manager:         helm
    Operation:       Update
    Time:            2023-02-22T21:15:21Z
  Resource Version:  691
  UID:               aa1ee360-bbea-4200-a6ad-bfeb7081818d
Spec:
  Groups:
    Name:  node.rules
    Rules:
      Expr:  topk by(cluster, namespace, pod) (1,
  max by (cluster, node, namespace, pod) (
    label_replace(kube_pod_info{job="kube-state-metrics",node!=""}, "pod", "$1", "pod", "(.*)")
))
      Record:  node_namespace_pod:kube_pod_info:
      Expr:    count by (cluster, node) (
  node_cpu_seconds_total{mode="idle",job="node-exporter"}
  * on (namespace, pod) group_left(node)
  topk by(namespace, pod) (1, node_namespace_pod:kube_pod_info:)
)
      Record:  node:node_num_cpu:sum
      Expr:    sum(
  node_memory_MemAvailable_bytes{job="node-exporter"} or
  (
    node_memory_Buffers_bytes{job="node-exporter"} +
    node_memory_Cached_bytes{job="node-exporter"} +
    node_memory_MemFree_bytes{job="node-exporter"} +
    node_memory_Slab_bytes{job="node-exporter"}
  )
) by (cluster)
      Record:  :node_memory_MemAvailable_bytes:sum
      Expr:    avg by (cluster, node) (
  sum without (mode) (
    rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal",job="node-exporter"}[5m])
  )
)
      Record:  node:node_cpu_utilization:ratio_rate5m
      Expr:    avg by (cluster) (
  node:node_cpu_utilization:ratio_rate5m
)
      Record:  cluster:node_cpu:ratio_rate5m
Events:        <none>


Name:         prometheus-kube-prometheus-prometheus
Namespace:    default
Labels:       app=kube-prometheus-stack
              app.kubernetes.io/instance=prometheus
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/part-of=kube-prometheus-stack
              app.kubernetes.io/version=45.2.0
              chart=kube-prometheus-stack-45.2.0
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-22T21:15:22Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"prometheus"}:
            .:
            f:name:
            f:rules:
    Manager:         helm
    Operation:       Update
    Time:            2023-02-22T21:15:21Z
  Resource Version:  677
  UID:               092d8dc5-9ff2-4a99-a405-f107eb3f928d
Spec:
  Groups:
    Name:  prometheus
    Rules:
      Alert:  PrometheusBadConfig
      Annotations:
        Description:  Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to reload its configuration.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusbadconfig
        Summary:      Failed Prometheus configuration reload.
      Expr:           # Without max_over_time, failed scrapes could create false negatives, see
# https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
max_over_time(prometheus_config_last_reload_successful{job="prometheus-kube-prometheus-prometheus",namespace="default"}[5m]) == 0
      For:  10m
      Labels:
        Severity:  critical
      Alert:       PrometheusNotificationQueueRunningFull
      Annotations:
        Description:  Alert notification queue of Prometheus {{$labels.namespace}}/{{$labels.pod}} is running full.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotificationqueuerunningfull
        Summary:      Prometheus alert notification queue predicted to run full in less than 30m.
      Expr:           # Without min_over_time, failed scrapes could create false negatives, see
# https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
(
  predict_linear(prometheus_notifications_queue_length{job="prometheus-kube-prometheus-prometheus",namespace="default"}[5m], 60 * 30)
>
  min_over_time(prometheus_notifications_queue_capacity{job="prometheus-kube-prometheus-prometheus",namespace="default"}[5m])
)
      For:  15m
      Labels:
        Severity:  warning
      Alert:       PrometheusErrorSendingAlertsToSomeAlertmanagers
      Annotations:
        Description:  {{ printf "%.1f" $value }}% errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.alertmanager}}.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuserrorsendingalertstosomealertmanagers
        Summary:      Prometheus has encountered more than 1% errors sending alerts to a specific Alertmanager.
      Expr:           (
  rate(prometheus_notifications_errors_total{job="prometheus-kube-prometheus-prometheus",namespace="default"}[5m])
/
  rate(prometheus_notifications_sent_total{job="prometheus-kube-prometheus-prometheus",namespace="default"}[5m])
)
* 100
> 1
      For:  15m
      Labels:
        Severity:  warning
      Alert:       PrometheusNotConnectedToAlertmanagers
      Annotations:
        Description:  Prometheus {{$labels.namespace}}/{{$labels.pod}} is not connected to any Alertmanagers.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotconnectedtoalertmanagers
        Summary:      Prometheus is not connected to any Alertmanagers.
      Expr:           # Without max_over_time, failed scrapes could create false negatives, see
# https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
max_over_time(prometheus_notifications_alertmanagers_discovered{job="prometheus-kube-prometheus-prometheus",namespace="default"}[5m]) < 1
      For:  10m
      Labels:
        Severity:  warning
      Alert:       PrometheusTSDBReloadsFailing
      Annotations:
        Description:  Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value | humanize}} reload failures over the last 3h.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustsdbreloadsfailing
        Summary:      Prometheus has issues reloading blocks from disk.
      Expr:           increase(prometheus_tsdb_reloads_failures_total{job="prometheus-kube-prometheus-prometheus",namespace="default"}[3h]) > 0
      For:            4h
      Labels:
        Severity:  warning
      Alert:       PrometheusTSDBCompactionsFailing
      Annotations:
        Description:  Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value | humanize}} compaction failures over the last 3h.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustsdbcompactionsfailing
        Summary:      Prometheus has issues compacting blocks.
      Expr:           increase(prometheus_tsdb_compactions_failed_total{job="prometheus-kube-prometheus-prometheus",namespace="default"}[3h]) > 0
      For:            4h
      Labels:
        Severity:  warning
      Alert:       PrometheusNotIngestingSamples
      Annotations:
        Description:  Prometheus {{$labels.namespace}}/{{$labels.pod}} is not ingesting samples.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotingestingsamples
        Summary:      Prometheus is not ingesting samples.
      Expr:           (
  rate(prometheus_tsdb_head_samples_appended_total{job="prometheus-kube-prometheus-prometheus",namespace="default"}[5m]) <= 0
and
  (
    sum without(scrape_job) (prometheus_target_metadata_cache_entries{job="prometheus-kube-prometheus-prometheus",namespace="default"}) > 0
  or
    sum without(rule_group) (prometheus_rule_group_rules{job="prometheus-kube-prometheus-prometheus",namespace="default"}) > 0
  )
)
      For:  10m
      Labels:
        Severity:  warning
      Alert:       PrometheusDuplicateTimestamps
      Annotations:
        Description:  Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf "%.4g" $value  }} samples/s with different values but duplicated timestamp.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusduplicatetimestamps
        Summary:      Prometheus is dropping samples with duplicate timestamps.
      Expr:           rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job="prometheus-kube-prometheus-prometheus",namespace="default"}[5m]) > 0
      For:            10m
      Labels:
        Severity:  warning
      Alert:       PrometheusOutOfOrderTimestamps
      Annotations:
        Description:  Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf "%.4g" $value  }} samples/s with timestamps arriving out of order.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusoutofordertimestamps
        Summary:      Prometheus drops samples with out-of-order timestamps.
      Expr:           rate(prometheus_target_scrapes_sample_out_of_order_total{job="prometheus-kube-prometheus-prometheus",namespace="default"}[5m]) > 0
      For:            10m
      Labels:
        Severity:  warning
      Alert:       PrometheusRemoteStorageFailures
      Annotations:
        Description:  Prometheus {{$labels.namespace}}/{{$labels.pod}} failed to send {{ printf "%.1f" $value }}% of the samples to {{ $labels.remote_name}}:{{ $labels.url }}
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotestoragefailures
        Summary:      Prometheus fails to send samples to remote storage.
      Expr:           (
  (rate(prometheus_remote_storage_failed_samples_total{job="prometheus-kube-prometheus-prometheus",namespace="default"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job="prometheus-kube-prometheus-prometheus",namespace="default"}[5m]))
/
  (
    (rate(prometheus_remote_storage_failed_samples_total{job="prometheus-kube-prometheus-prometheus",namespace="default"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job="prometheus-kube-prometheus-prometheus",namespace="default"}[5m]))
  +
    (rate(prometheus_remote_storage_succeeded_samples_total{job="prometheus-kube-prometheus-prometheus",namespace="default"}[5m]) or rate(prometheus_remote_storage_samples_total{job="prometheus-kube-prometheus-prometheus",namespace="default"}[5m]))
  )
)
* 100
> 1
      For:  15m
      Labels:
        Severity:  critical
      Alert:       PrometheusRemoteWriteBehind
      Annotations:
        Description:  Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write is {{ printf "%.1f" $value }}s behind for {{ $labels.remote_name}}:{{ $labels.url }}.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotewritebehind
        Summary:      Prometheus remote write is behind.
      Expr:           # Without max_over_time, failed scrapes could create false negatives, see
# https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
(
  max_over_time(prometheus_remote_storage_highest_timestamp_in_seconds{job="prometheus-kube-prometheus-prometheus",namespace="default"}[5m])
- ignoring(remote_name, url) group_right
  max_over_time(prometheus_remote_storage_queue_highest_sent_timestamp_seconds{job="prometheus-kube-prometheus-prometheus",namespace="default"}[5m])
)
> 120
      For:  15m
      Labels:
        Severity:  critical
      Alert:       PrometheusRemoteWriteDesiredShards
      Annotations:
        Description:  Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write desired shards calculation wants to run {{ $value }} shards for queue {{ $labels.remote_name}}:{{ $labels.url }}, which is more than the max of {{ printf `prometheus_remote_storage_shards_max{instance="%s",job="prometheus-kube-prometheus-prometheus",namespace="default"}` $labels.instance | query | first | value }}.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotewritedesiredshards
        Summary:      Prometheus remote write desired shards calculation wants to run more than configured max shards.
      Expr:           # Without max_over_time, failed scrapes could create false negatives, see
# https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
(
  max_over_time(prometheus_remote_storage_shards_desired{job="prometheus-kube-prometheus-prometheus",namespace="default"}[5m])
>
  max_over_time(prometheus_remote_storage_shards_max{job="prometheus-kube-prometheus-prometheus",namespace="default"}[5m])
)
      For:  15m
      Labels:
        Severity:  warning
      Alert:       PrometheusRuleFailures
      Annotations:
        Description:  Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to evaluate {{ printf "%.0f" $value }} rules in the last 5m.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusrulefailures
        Summary:      Prometheus is failing rule evaluations.
      Expr:           increase(prometheus_rule_evaluation_failures_total{job="prometheus-kube-prometheus-prometheus",namespace="default"}[5m]) > 0
      For:            15m
      Labels:
        Severity:  critical
      Alert:       PrometheusMissingRuleEvaluations
      Annotations:
        Description:  Prometheus {{$labels.namespace}}/{{$labels.pod}} has missed {{ printf "%.0f" $value }} rule group evaluations in the last 5m.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusmissingruleevaluations
        Summary:      Prometheus is missing rule evaluations due to slow rule group evaluation.
      Expr:           increase(prometheus_rule_group_iterations_missed_total{job="prometheus-kube-prometheus-prometheus",namespace="default"}[5m]) > 0
      For:            15m
      Labels:
        Severity:  warning
      Alert:       PrometheusTargetLimitHit
      Annotations:
        Description:  Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped {{ printf "%.0f" $value }} targets because the number of targets exceeded the configured target_limit.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustargetlimithit
        Summary:      Prometheus has dropped targets because some scrape configs have exceeded the targets limit.
      Expr:           increase(prometheus_target_scrape_pool_exceeded_target_limit_total{job="prometheus-kube-prometheus-prometheus",namespace="default"}[5m]) > 0
      For:            15m
      Labels:
        Severity:  warning
      Alert:       PrometheusLabelLimitHit
      Annotations:
        Description:  Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped {{ printf "%.0f" $value }} targets because some samples exceeded the configured label_limit, label_name_length_limit or label_value_length_limit.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuslabellimithit
        Summary:      Prometheus has dropped targets because some scrape configs have exceeded the labels limit.
      Expr:           increase(prometheus_target_scrape_pool_exceeded_label_limits_total{job="prometheus-kube-prometheus-prometheus",namespace="default"}[5m]) > 0
      For:            15m
      Labels:
        Severity:  warning
      Alert:       PrometheusScrapeBodySizeLimitHit
      Annotations:
        Description:  Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed {{ printf "%.0f" $value }} scrapes in the last 5m because some targets exceeded the configured body_size_limit.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusscrapebodysizelimithit
        Summary:      Prometheus has dropped some targets that exceeded body size limit.
      Expr:           increase(prometheus_target_scrapes_exceeded_body_size_limit_total{job="prometheus-kube-prometheus-prometheus",namespace="default"}[5m]) > 0
      For:            15m
      Labels:
        Severity:  warning
      Alert:       PrometheusScrapeSampleLimitHit
      Annotations:
        Description:  Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed {{ printf "%.0f" $value }} scrapes in the last 5m because some targets exceeded the configured sample_limit.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusscrapesamplelimithit
        Summary:      Prometheus has failed scrapes that have exceeded the configured sample limit.
      Expr:           increase(prometheus_target_scrapes_exceeded_sample_limit_total{job="prometheus-kube-prometheus-prometheus",namespace="default"}[5m]) > 0
      For:            15m
      Labels:
        Severity:  warning
      Alert:       PrometheusTargetSyncFailure
      Annotations:
        Description:  {{ printf "%.0f" $value }} targets in Prometheus {{$labels.namespace}}/{{$labels.pod}} have failed to sync because invalid configuration was supplied.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustargetsyncfailure
        Summary:      Prometheus has failed to sync targets.
      Expr:           increase(prometheus_target_sync_failed_total{job="prometheus-kube-prometheus-prometheus",namespace="default"}[30m]) > 0
      For:            5m
      Labels:
        Severity:  critical
      Alert:       PrometheusHighQueryLoad
      Annotations:
        Description:  Prometheus {{$labels.namespace}}/{{$labels.pod}} query API has less than 20% available capacity in its query engine for the last 15 minutes.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheushighqueryload
        Summary:      Prometheus is reaching its maximum capacity serving concurrent requests.
      Expr:           avg_over_time(prometheus_engine_queries{job="prometheus-kube-prometheus-prometheus",namespace="default"}[5m]) / max_over_time(prometheus_engine_queries_concurrent_max{job="prometheus-kube-prometheus-prometheus",namespace="default"}[5m]) > 0.8
      For:            15m
      Labels:
        Severity:  warning
      Alert:       PrometheusErrorSendingAlertsToAnyAlertmanager
      Annotations:
        Description:  {{ printf "%.1f" $value }}% minimum errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to any Alertmanager.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuserrorsendingalertstoanyalertmanager
        Summary:      Prometheus encounters more than 3% errors sending alerts to any Alertmanager.
      Expr:           min without (alertmanager) (
  rate(prometheus_notifications_errors_total{job="prometheus-kube-prometheus-prometheus",namespace="default",alertmanager!~``}[5m])
/
  rate(prometheus_notifications_sent_total{job="prometheus-kube-prometheus-prometheus",namespace="default",alertmanager!~``}[5m])
)
* 100
> 3
      For:  15m
      Labels:
        Severity:  critical
Events:            <none>


Name:         prometheus-kube-prometheus-prometheus-operator
Namespace:    default
Labels:       app=kube-prometheus-stack
              app.kubernetes.io/instance=prometheus
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/part-of=kube-prometheus-stack
              app.kubernetes.io/version=45.2.0
              chart=kube-prometheus-stack-45.2.0
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2023-02-22T21:15:22Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app:
          f:app.kubernetes.io/instance:
          f:app.kubernetes.io/managed-by:
          f:app.kubernetes.io/part-of:
          f:app.kubernetes.io/version:
          f:chart:
          f:heritage:
          f:release:
      f:spec:
        .:
        f:groups:
          .:
          k:{"name":"prometheus-operator"}:
            .:
            f:name:
            f:rules:
    Manager:         helm
    Operation:       Update
    Time:            2023-02-22T21:15:21Z
  Resource Version:  697
  UID:               706be8a5-598c-402a-8dc6-16b4ff7eca54
Spec:
  Groups:
    Name:  prometheus-operator
    Rules:
      Alert:  PrometheusOperatorListErrors
      Annotations:
        Description:  Errors while performing List operations in controller {{$labels.controller}} in {{$labels.namespace}} namespace.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorlisterrors
        Summary:      Errors while performing list operations in controller.
      Expr:           (sum by (controller,namespace,cluster) (rate(prometheus_operator_list_operations_failed_total{job="prometheus-kube-prometheus-operator",namespace="default"}[10m])) / sum by (controller,namespace,cluster) (rate(prometheus_operator_list_operations_total{job="prometheus-kube-prometheus-operator",namespace="default"}[10m]))) > 0.4
      For:            15m
      Labels:
        Severity:  warning
      Alert:       PrometheusOperatorWatchErrors
      Annotations:
        Description:  Errors while performing watch operations in controller {{$labels.controller}} in {{$labels.namespace}} namespace.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorwatcherrors
        Summary:      Errors while performing watch operations in controller.
      Expr:           (sum by (controller,namespace,cluster) (rate(prometheus_operator_watch_operations_failed_total{job="prometheus-kube-prometheus-operator",namespace="default"}[5m])) / sum by (controller,namespace,cluster) (rate(prometheus_operator_watch_operations_total{job="prometheus-kube-prometheus-operator",namespace="default"}[5m]))) > 0.4
      For:            15m
      Labels:
        Severity:  warning
      Alert:       PrometheusOperatorSyncFailed
      Annotations:
        Description:  Controller {{ $labels.controller }} in {{ $labels.namespace }} namespace fails to reconcile {{ $value }} objects.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorsyncfailed
        Summary:      Last controller reconciliation failed
      Expr:           min_over_time(prometheus_operator_syncs{status="failed",job="prometheus-kube-prometheus-operator",namespace="default"}[5m]) > 0
      For:            10m
      Labels:
        Severity:  warning
      Alert:       PrometheusOperatorReconcileErrors
      Annotations:
        Description:  {{ $value | humanizePercentage }} of reconciling operations failed for {{ $labels.controller }} controller in {{ $labels.namespace }} namespace.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorreconcileerrors
        Summary:      Errors while reconciling controller.
      Expr:           (sum by (controller,namespace,cluster) (rate(prometheus_operator_reconcile_errors_total{job="prometheus-kube-prometheus-operator",namespace="default"}[5m]))) / (sum by (controller,namespace,cluster) (rate(prometheus_operator_reconcile_operations_total{job="prometheus-kube-prometheus-operator",namespace="default"}[5m]))) > 0.1
      For:            10m
      Labels:
        Severity:  warning
      Alert:       PrometheusOperatorNodeLookupErrors
      Annotations:
        Description:  Errors while reconciling Prometheus in {{ $labels.namespace }} Namespace.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatornodelookuperrors
        Summary:      Errors while reconciling Prometheus.
      Expr:           rate(prometheus_operator_node_address_lookup_errors_total{job="prometheus-kube-prometheus-operator",namespace="default"}[5m]) > 0.1
      For:            10m
      Labels:
        Severity:  warning
      Alert:       PrometheusOperatorNotReady
      Annotations:
        Description:  Prometheus operator in {{ $labels.namespace }} namespace isn't ready to reconcile {{ $labels.controller }} resources.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatornotready
        Summary:      Prometheus operator not ready
      Expr:           min by (controller,namespace,cluster) (max_over_time(prometheus_operator_ready{job="prometheus-kube-prometheus-operator",namespace="default"}[5m]) == 0)
      For:            5m
      Labels:
        Severity:  warning
      Alert:       PrometheusOperatorRejectedResources
      Annotations:
        Description:  Prometheus operator in {{ $labels.namespace }} namespace rejected {{ printf "%0.0f" $value }} {{ $labels.controller }}/{{ $labels.resource }} resources.
        runbook_url:  https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorrejectedresources
        Summary:      Resources rejected by Prometheus operator
      Expr:           min_over_time(prometheus_operator_managed_resources{state="rejected",job="prometheus-kube-prometheus-operator",namespace="default"}[5m]) > 0
      For:            5m
      Labels:
        Severity:  warning
Events:            <none>
